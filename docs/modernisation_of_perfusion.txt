\documentclass{article}
\usepackage{fvextra}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{newunicodechar}
\newunicodechar{├}{|}{└──}
\usepackage{multicol}
\usepackage{listingsutf8}
\usepackage{subcaption}

% Define YAML language for listings
\lstdefinelanguage{yaml}{
  morekeywords={true,false,null,y,n},
  sensitive=false,
  morecomment=[l]{\#},
  morestring=[b]",
  morestring=[b]'
}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{gray!5},
  commentstyle=\color{gray!60},
  keywordstyle=\color{blue!90},
  stringstyle=\color{orange!90!black},
  basicstyle=\ttfamily\small,
  breaklines=true,
  captionpos=b,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=4
}
\lstset{style=mystyle}

% Bibliography
\usepackage{biblatex}
\addbibresource{references.bib}

% Optional polish
\usepackage{microtype}    % Improves justification
\usepackage{geometry}     % Easy margin adjustments
\usepackage{fancyhdr}     % Custom headers/footers
\usepackage{csquotes}     % Recommended with biblatex



\title{Modernization, and User Guide of Project Geminis Perfusion Model}
\author{Natalia Denton}
\date{April 2025}

\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Overview of Perfusion Model}

This section outlines the structure and execution workflow of the cerebral perfusion simulation pipeline. The model simulates blood flow through brain tissue using a multi-compartment porous medium framework, implemented in FEniCSx. The simulation comprises three main stages: pre-processing of anatomical meshes, initialization of permeability tensors, and numerical solution of the pressure and velocity fields under healthy or occluded conditions. The pipeline is fully configurable using human-readable YAML files, which define all physical, numerical, and boundary parameters. The following subsections describe each stage in detail and provide step-by-step instructions for execution.

This code was modenised from fenics-legacy to FEniCSx to ensure long-term maintainability, improved scalability, and compatibility with evolving scientific computing standards, the computational framework originally developed using legacy \texttt{FEniCS} (\texttt{dolfin}, \texttt{ufl}, \texttt{mshr}) was systematically translated to the modern \texttt{FEniCSx} stack. The newer platform, based on \texttt{dolfinx}, \texttt{ufl}, \texttt{basix}, and \texttt{petsc4py}, offers greater modularity, direct MPI integration, and lower level control over finite element and linear algebra backends~\cite{fenicsxDocs}.


\section{Repository Structure Overview}

To facilitate reproducibility and ease of navigation, the full directory structure of the project is provided below.
This structure corresponds to the repository as of April 2025 and includes scripts, configuration files, mesh data, testing scripts, container definitions, and documentation resources.

Users should preserve this structure to ensure correct functionality of YAML configuration files, script execution, and MPI job submissions.
\newpage
\vspace{1em}

\begin{multicols}{2}
\begin{verbatim}[language=bash]
├── README.md
├── VP_results
│   └── p0000
├── brain_meshes
│   └── b0000
│       ├── affine_matrices.yaml
│       ├── boundary_mapper.csv
│       ├── clustered.h5
│       ├── clustered.xdmf
│       ├── clustered_facet_region.h5
│       ├── clustered_facet_region.xdmf
│       ├── clustered_physical_region.h5
│       ├── clustered_physical_region.xdmf
│       └── permeability
│           ├── K1_form.h5
│           ├── K1_form.xdmf
│           ├── e_loc.h5
│           ├── e_loc.xdmf
│           ├── main_direction.h5
│           └── main_direction.xdmf
├── containers
│   ├── container.def
│   └── new_container.def
├── doc
│   ├── Makefile
│   ├── _build
│   │   ├── doctrees
│   │   │   ├── environment.pickle
│   │   │   └── ...
│   │   └── html
│   │       ├── _sources
│   │       │   └── ...
│   │       ├── _static
│   │       │   └── ...
│   │       └── files
│   ├── fidel_files
│   │   ├── FEniCSx_Oxygen_Model_Test_Form.pdf
│   │   └── ...
│   ├── files
│   ├── g2_report
│   ├── html
│   └── index.rst
├── file_path_finder.py
├── perfusion
│   └── weak_vs_strong
├── run_this_file.py
├── scripts
│   ├── Ares
│   │   └── ...
│   ├── run_files
│   └── sub_scripts
├── src
│   ├── Gem_Legacy
│   │   ├── Dockerfile
│   │   ├── brain_meshes.tar.xz
│   │   ├── perfusion
│   │   │   ├── API.py
│   │   │   ├── config_examples
│   │   │   ├── verification
│   │   │   └── weak_vs_strong
│   │   ├── oxygen
│   │   └── tissue_health
│   └── Gem_X
│       ├── API.py
│       ├── configurations
│       │   ├── config_examples
│       │   └── tissue_health
│       ├── core
│       │   ├── oxygen
│       │   ├── perfusion
│       │   └── sensitivity
│       ├── functions
│       ├── post_processing
│       └── runners
└── tests
    └── text.txt
\end{verbatim}
\end{multicols}

\vspace{1em}

\textbf{Legend:}
\begin{itemize}
    \item \texttt{brain\_meshes/}: Contains all mesh files, permeability tensors, and boundary data.
    \item \texttt{doc/}: Documentation build and source files using Sphinx.
    \item \texttt{perfusion/}: Scripts related to strong vs weak scaling tests.
    \item \texttt{scripts/}: Job submission scripts for HPC execution.
    \item \texttt{src/}: Main source code for Legacy and Modernised (Gem\_X) models.
    \item \texttt{tests/}: Test scripts and additional resources.
    \item \texttt{containers/}: Container definition files (Singularity/Apptainer).
\end{itemize}


\section{Running the Perfusion Model}

\subsection{Requirements}

To ensure reproducibility and compatibility, all simulations were conducted using the following software environment (as shown by \texttt{pip list}). Only the packages directly used in the mesh preprocessing, permeability initialisation, boundary‐condition generation, and flow solvers are listed.

\paragraph{Core FEniCSx Stack}
\begin{itemize}
  \item \textbf{fenics-basix} 0.9.0 \cite{fenics_basix}
  \item \textbf{fenics-dolfinx} 0.9.0 \cite{fenics_dolfinx}
  \item \textbf{fenics-ffcx} 0.9.0 \cite{fenics_ffcx}
  \item \textbf{fenics-ufl} 2024.2.0 \cite{fenics_ufl}
\end{itemize}

\paragraph{Linear Algebra / MPI}
\begin{itemize}
  \item \textbf{petsc4py} 3.22.3 \cite{petsc4py}  (PETSc backend for solvers)
  \item \textbf{mpi4py} 4.0.3 \cite{mpi4py}    (MPI communication)
  \item \textbf{h5py} 3.13.0 \cite{h5py}    (HDF5 I/O for meshes and results)
\end{itemize}

\paragraph{Scientific Python Ecosystem}
\begin{itemize}
  \item \textbf{numpy} 2.2.3 \cite{numpy}    (array manipulations)
  \item \textbf{scipy} 1.15.2 \cite{scipy}   (numerical utilities)
  \item \textbf{pandas} 2.2.3 \cite{pandas}   (CSV handling in \texttt{BC\_creator.py})
  \item \textbf{PyYAML} 6.0.2 \cite{pyyaml}   (YAML configuration parsing)
\end{itemize}

\paragraph{Utilities / Postprocessing}
\begin{itemize}
  \item \textbf{joblib} 1.4.2 \cite{joblib}   (optional parallel loops)
  \item \textbf{tqdm} 4.67.1 \cite{tqdm}    (progress bars in long loops)
\end{itemize}

\paragraph{Execution Environment}
\begin{itemize}
  \item \textbf{Python} 3.10.x \cite{python}
  \item \textbf{MPI} — OpenMPI / MPICH 3.x or higher
  \item \textbf{Linux/Unix} with standard shell tools
\end{itemize}

\paragraph{Documentation and Compatibility References}

For further details on the core components of the FEniCSx stack and their documentation, refer to the following sources:

\begin{itemize}
  \item \textbf{dolfinx}: Python API documentation for \texttt{dolfinx} (v0.9.0) \cite{fenics_dolfinx}.
  \item \textbf{FFCx}: Compiler documentation for the FEniCSx FFC compiler (v0.9.0) \cite{fenics_ffcx}.
  \item \textbf{UFL}: Unified Form Language (UFL) documentation (v2024.2.0) \cite{fenics_ufl}.
  \item \textbf{Basix}: Documentation for the Basix finite element library (v0.9.0) \cite{fenics_basix}.
\end{itemize}



\paragraph{Containerisation (Recommended)}
For maximal reproducibility, it is recommended to encapsulate this environment in a Docker or Singularity container specifying the above versions. This ensures consistent behavior across different compute nodes and reduces setup overhead on HPC systems. The def files for the containers are given within the perfusion file with instructions of use.


\subsection{Perfusion Pipeline}
\begin{itemize}
    \item Extract the \texttt{brain\_meshes.tar.xz} archive, and place its contents in the main project directory.

    \item Compute the permeability tensor by running \texttt{permeability\_initialiser.py}. For parallel execution, use:
    \begin{lstlisting}[language=bash]
mpirun -n <number_of_processors> python3 permeability_initialiser.py
    \end{lstlisting}
    With 4 cores, execution typically completes in under 2 minutes. Parameters are read from the \texttt{config\_permeability\_initialiser.yaml} file. This script only needs to be run once, unless permeability configuration parameters or mesh files are changed—then the tensor must be re-initialised.

    \item Solve for the pressure and velocity fields using \texttt{basic\_flow\_solver.py}. The solver supports parallel execution for healthy perfusion simulations. Example command:
    \begin{lstlisting}[language=bash]
mpirun -n <number_of_processors> python3 basic_flow_solver.py
    \end{lstlisting}
    Using 4 cores and first-order finite elements, the simulation typically completes in under 2 minutes. Parameters are defined in \texttt{config\_basic\_flow\_solver.yaml}.

    \item For occluded scenarios, a boundary condition file must be provided to specify pressure and/or volumetric flow rate on cortical surface territories. An example for right MCA occlusion is included as \texttt{BC\_template.csv}. Similar files can be generated by running \texttt{BC\_creator.py} (in serial):
    \begin{lstlisting}[language=bash]
python3 BC_creator.py
    \end{lstlisting}
    After generating this file, re-run the solver with updated YAML configuration pointing to the new inlet boundary file. Ensure that the \texttt{output/res\_fldr} path is changed to avoid overwriting results from previous simulations.

    \item If you would like to run Neumann Boundary Conditions or mixed conditions for a healthy/non-occluded scenario, the BC\_creator.py must be run. The BC\_creator.py uses the same config file as the basic flow solver. if you intend to change the BCs or flow characteristics or occlusion. the BC\_creator.py must be run each time  
\end{itemize}

\vspace{1em}

The \texttt{.csv} file summarising the boundary conditions uses the following surface region IDs for cortical territories:
\begin{itemize}
    \item[21] Left anterior cerebral artery (ACA)
    \item[22] Left middle cerebral artery (MCA)
    \item[23] Left posterior cerebral artery (PCA)
    \item[24] Right anterior cerebral artery (ACA)
    \item[25] Right middle cerebral artery (MCA)
    \item[26] Right posterior cerebral artery (PCA)
    \item[30] Brain stem
    \item[4] Cerebellar artery
\end{itemize}


\subsection{Configuration files}

\subsubsection{Modernized Occlusion Handling via Configuration File}

In the legacy \texttt{FEniCS} implementation, occlusion scenarios were manually specified through command-line arguments or by modifying the Python source code. For example, a right MCA occlusion would be triggered by executing:

\begin{lstlisting}[language=bash, caption=Legacy command-line execution]
python3 BC_creator.py --occluded --occl_ID 25 --mesh_file path/to/mesh.xdmf
\end{lstlisting}

This approach relied on \texttt{argparse} to handle parameter input at runtime. While functional, it suffered from several usability issues:
\begin{itemize}
    \item Required users to remember and manually supply all flags at each run
    \item Increased the likelihood of syntax or logic errors due to missing arguments
    \item Coupled execution logic tightly with parameter values, reducing script modularity
    \item Made it difficult to reproduce simulations or conduct batch studies across occlusion cases
\end{itemize}

In contrast, the updated configuration system in the \texttt{FEniCSx} implementation encapsulates all relevant inputs in a single YAML configuration file. Occlusion status and arterial selection are defined explicitly using simple flags and lists:

\begin{lstlisting}[language=yaml, caption=Occlusion-specific fields in FEniCSx configuration]
input:
  healthy: False
  occl_ID: [25]
  read_inlet_boundary: true
  inlet_boundary_file: boundary_data/BCs_RMCA.csv
\end{lstlisting}

This design improves usability and maintainability by separating configuration from code logic. The YAML file is human-readable, version-controllable, and fully reproducible. The benefits of this approach include:

\begin{itemize}
    \item \textbf{Explicit case control:} The \texttt{healthy} flag toggles between healthy and pathological simulations, making intent clear to both users and developers.
    \item \textbf{Flexible occlusion specification:} Arterial occlusion is defined through the \texttt{occl\_ID} list, which can easily be modified for testing different clinical scenarios.
    \item \textbf{Transparent boundary conditions:} The \texttt{inlet\_boundary\_file} points to a CSV file containing surface region assignments, pressures, and flow rates—fully decoupled from the core solver.
    \item \textbf{Improved reproducibility:} Entire simulation setups are defined in a single file, allowing researchers to re-run or share scenarios without relying on memory or external documentation.
    \item \textbf{Ease of automation:} The new system is compatible with batch scripts, parameter sweeps, and HPC job arrays, making it ideal for large-scale testing and optimization.
\end{itemize}

Overall, the transition from command-line argument parsing to YAML-based configuration aligns the codebase with modern scientific software design principles. It empowers users with a more intuitive, scalable, and reproducible method of running occlusion scenarios.

\subsubsection{How to Use the Permeability Initialiser Configuration File}

The generation of the permeability tensor fields for grey and white matter tissues is handled by the \texttt{permeability\_initialiser.py} script. This script reads the simulation mesh, computes directional tensor values based on surface orientation, and writes the resulting data into an HDF5 or XDMF-compatible format.

All configuration parameters for this step are stored in a dedicated YAML file—typically named \texttt{config\_permeability\_initialiser.yaml}.

The structure of this file includes three blocks: \texttt{input}, \texttt{output}, and \texttt{physical}.

\paragraph{1. \texttt{input}: Mesh file}

This block specifies the spatial domain to be used for the tensor field computation:

\begin{lstlisting}[language=yaml]
input:
  mesh_file: '../brain_meshes/b0000/clustered.xdmf'
\end{lstlisting}

\textbf{Key point:}
\begin{itemize}
    \item \texttt{mesh\_file}: Path to the labelled tetrahedral mesh file containing cortical surface and tissue regions. The mesh must include appropriate boundary markers to infer surface normals.
\end{itemize}

\paragraph{2. \texttt{output}: Result storage and resolution settings}

This section defines where to store the resulting tensor field and whether or not to save intermediate outputs:

\begin{lstlisting}[language=yaml]
output:
  res_fldr: '../brain_meshes/b0000/permeability/'
  save_subres: false
  res_vars: {'K1_form'}
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{res\_fldr}: Target directory where the generated permeability tensors will be saved.
    \item \texttt{save\_subres}: Boolean flag for saving additional intermediate results (e.g., gradients, masks). Set to \texttt{true} only for debugging or detailed analysis.
    \item \texttt{res\_vars}: List of result variables to compute. Currently supports \texttt{K1\_form}, which refers to the tensor field for arteriolar (or venular) permeability.
\end{itemize}

\paragraph{3. \texttt{physical}: Tensor definition and anatomical reference}

This block encodes the directional properties and magnitude of the permeability tensor:

\begin{lstlisting}[language=yaml]
physical:
  e_ref: [0, 0, 1]
  K1_form: [0, 0, 0, 0, 0, 0, 0, 0, 1]
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{e\_ref}: The reference normal vector (typically the $z$-axis) used to infer orientation of the cortical surface in the brain. This helps distinguish radial from tangential directions.
    \item \texttt{K1\_form}: A 3x3 tensor stored in flattened row-major format. This tensor defines the reference permeability in each coordinate direction. The example above represents an identity matrix along the $z$-axis, suggesting dominant flow perpendicular to the cortical surface.
\end{itemize}

\paragraph{Usage Tips}

\begin{itemize}
    \item Ensure the mesh includes clearly defined surface normals and boundary facets; otherwise, the orientation inference may be inaccurate.
    \item For anisotropic configurations (e.g., tangential-only flow), adjust the \texttt{K1\_form} to represent off-diagonal or planar-dominant tensors.
    \item Always re-run the initialiser if any of the physical constants, reference orientation, or resolution settings change.
    \item Results from this stage are consumed by the basic flow solver during simulation, so consistency between mesh versions is critical.
\end{itemize}

By defining permeability fields externally and configurably, this component of the model promotes a clean separation between geometry processing and numerical solving. It also allows easy experimentation with tissue anisotropy, vascular orientation, and alternative permeability hypotheses.


\subsubsection{How to Use the Basic Flow Solver Configuration Files}

The simulation is controlled entirely via YAML-based configuration files. This modular setup allows different components of the workflow to be parameterised independently of the core code logic. The main configuration file for pressure and velocity simulation is \texttt{config\_basic\_flow\_solver.yaml}.

This file is divided into five logical blocks: \texttt{input}, \texttt{output}, \texttt{physical}, \texttt{simulation}, and \texttt{optimisation}. Each block governs a specific part of the simulation.

\paragraph{1. \texttt{input}: Data paths and simulation mode}

The \texttt{input} section specifies the location of the mesh, inlet boundary file, and precomputed permeability tensor, as well as the physiological state (healthy vs. occluded):

\begin{lstlisting}[language=yaml]
input:
  healthy: False
  occl_ID: [25]
  read_inlet_boundary: true
  inlet_boundary_file: boundary_data/BCs_RMCA.csv
  mesh_file: ../brain_meshes/b0000/clustered.xdmf
  permeability_folder: ../brain_meshes/b0000/permeability/
  inlet_BC_type: DBC
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{healthy}: Boolean flag to switch between healthy and occluded scenarios. Set to \texttt{false} to trigger occlusion logic.
    \item \texttt{occl\_ID}: List of occluded artery IDs. These must match surface labels defined in the mesh or CSV.
    \item \texttt{read\_inlet\_boundary}: Enables reading inlet boundary data from a CSV file (e.g., pressure and flow rates per cortical surface).
    \item \texttt{inlet\_BC\_type}: Used only when \texttt{read\_inlet\_boundary = false}; can be \texttt{DBC}, \texttt{NBC}, or \texttt{MIXED}.
\end{itemize}

\paragraph{2. \texttt{output}: Simulation results configuration}

The \texttt{output} section controls where results are saved and which quantities are written to disk:

\begin{lstlisting}[language=yaml]
output:
  comp_ave: false
  res_fldr: ../VP_results/p0000/a/DBC/healthy/read_inlet_true/FE_degree_1/np8/
  res_vars: {'press1','press2','press3','vel1','vel2','perfusion','beta12','beta23'}
  integral_vars: {}
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{res\_fldr}: Specifies the output directory for result files. This must be changed between simulations to avoid overwriting.
    \item \texttt{res\_vars}: List of variables to be saved, including pressure, velocity, perfusion, and coupling coefficients.
    \item \texttt{integral\_vars}: Optional integral quantities (e.g., \texttt{vel1\_surfint}, \texttt{press1\_surfint}) for evaluating physical properties.
\end{itemize}

\paragraph{3. \texttt{physical}: Physiological and boundary parameters}

This block defines physical constants for the cerebral perfusion model:

\begin{lstlisting}[language=yaml]
physical:
  K1gm_ref: 0.001234
  beta12gm: 1.326e-06
  p_arterial: 10000.0
  p_venous: 0.0
  Q_brain: 10.0
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{K1gm\_ref}, \texttt{K2gm\_ref}, etc.: Permeability values for each compartment (arteriole, capillary, venule).
    \item \texttt{beta12gm}, \texttt{beta23gm}: Coupling coefficients between compartments.
    \item \texttt{p\_arterial} and \texttt{p\_venous}: Used when Dirichlet pressure conditions are applied at inlets and outlets.
    \item \texttt{Q\_brain}: Total inflow rate used when applying Neumann or mixed boundary conditions.
\end{itemize}

\paragraph{4. \texttt{simulation}: Discretisation and model type}

This section specifies the numerical discretisation settings and model structure:

\begin{lstlisting}[language=yaml]
simulation:
  fe_degr: 1
  model_type: 'a'
  vel_order: 1
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{fe\_degr}: Degree of the finite element basis for pressure fields.
    \item \texttt{vel\_order}: Order used for velocity field projection.
    \item \texttt{model\_type}: Either \texttt{'a'} for single-compartment (arteriole only) or \texttt{'acv'} for full 3-compartment model.
\end{itemize}

\paragraph{5. \texttt{optimisation}: Optional parameter tuning}

If enabled, this block allows optimisation of key physical parameters:

\begin{lstlisting}[language=yaml]
optimisation:
  parameters: ['gmowm_beta_rat','K1gm_ref']
  random_init: true
  init_param_range: [[0.1,10],[0.0001,0.01]]
\end{lstlisting}

\textbf{Key points:}
\begin{itemize}
    \item \texttt{parameters}: List of parameters to optimise (e.g., GM/WM ratios, permeability).
    \item \texttt{random\_init}: If true, parameter values are initialised randomly within the specified range.
    \item \texttt{init\_param\_range}: Range of values allowed for each parameter.
\end{itemize}

\paragraph{Usage Tips}

\begin{itemize}
    \item Maintain separate config files for healthy and occluded scenarios.
    \item Use meaningful folder names in \texttt{res\_fldr} to avoid overwriting results.
    \item Ensure consistency between \texttt{occl\_ID} and the IDs defined in your mesh or CSV.
    \item Consider using symbolic links or batch scripts to swap configurations programmatically.
\end{itemize}

This YAML-driven configuration design provides a clear, user-friendly interface for defining complex simulations without modifying core source code. It improves reproducibility, modularity, and compatibility with high-performance computing workflows.


\section{Modernisation from FEniCS-Legacy to FEniCSx-0.9}

The perfusion model comprises three “runner” scripts: \texttt{permeability\_initialiser.py}, \texttt{basic\_flow\_solver.py}, and \texttt{BC\_creator.py}. These invoke our supporting libraries (\texttt{IO\_fcts.py}, \texttt{suppl\_fcts.py}, and \texttt{finite\_element\_fcts.py}).  In this section, we illustrate how each runner was modernised to use \texttt{dolfinx-0.9}, providing a direct translation guide.

\subsection{Permeability Initialiser}

This script computes an anisotropic permeability tensor field aligned with local vessel orientations. Below we compare key components of the legacy and FEniCSx-0.9 implementations.

\subsubsection{Logging Configuration}

\textbf{Legacy \texttt{dolfin}}:
\begin{lstlisting}[language=Python,caption=Legacy log suppression]
from dolfin import *
set_log_level(50)
\end{lstlisting}

\textbf{Modern \texttt{dolfinx}}:
\begin{lstlisting}[language=Python,caption=FEniCSx log suppression~\cite{dolfinxPyDocs}]
from dolfinx import log
log.set_log_level(log.LogLevel.WARNING)
\end{lstlisting}

\subsubsection{Finite Element Space Definition}

\textbf{Legacy \texttt{dolfin}}:
\begin{lstlisting}[language=Python,caption=Legacy tensor function space]
K_space = TensorFunctionSpace(mesh, "DG", 0)
\end{lstlisting}

\textbf{Modern \texttt{dolfinx}}:
\begin{lstlisting}[language=Python,caption=FEniCSx tensor-valued space using Basix~\cite{basixDocs}]
import basix.ufl
element_tensor = basix.ufl.element("DG", "tetrahedron", 0, shape=(mesh.geometry.dim, mesh.geometry.dim))
K_space = fem.functionspace(mesh, element_tensor)
\end{lstlisting}

\subsubsection{MPI‑Aware Output}

\textbf{Legacy \texttt{dolfin}}:
\begin{lstlisting}[language=Python,caption=Legacy rank‑guarded prints]
if rank == 0:
    print("Step 1: Reading input files")
\end{lstlisting}

\textbf{Modern \texttt{dolfinx}}:
\begin{lstlisting}[language=Python,caption=FEniCSx print0 helper]
from IO_fcts import print0
print0("Step 1: Reading input files")
\end{lstlisting}

\subsubsection{I/O and Data Saving}

\textbf{Legacy \texttt{dolfin}}:
\begin{lstlisting}[language=Python,caption=Legacy XDMF checkpoint writes]
with XDMFFile(path + 'K1_form.xdmf') as file:
    file.write_checkpoint(K1, "K1_form", 0, XDMFFile.Encoding.HDF5, False)
\end{lstlisting}

\textbf{Modern \texttt{dolfinx}}:
\begin{lstlisting}[language=Python,caption=FEniCSx write\_function with explicit mesh]
from dolfinx.io import XDMFFile
with XDMFFile(mesh.comm, path + 'K1_form.xdmf', "w",
              encoding=XDMFFile.Encoding.HDF5) as xdmf:
    xdmf.write_mesh(mesh)
    xdmf.write_function(K1, 0)
\end{lstlisting}

Each of these updates preserves the original algorithmic flow—reading the mesh, computing vessel orientation, assembling the tensor, and writing results—while adopting the explicit, MPI‑aware, and Basix‑driven APIs of \texttt{dolfinx-0.9}. This pattern is repeated across the other runner scripts, ensuring a uniform transition from legacy to modern code.


\subsection{Basic Flow Solver}

The \texttt{basic\_flow\_solver.py} script sets up and solves the multi‐compartment Darcy flow problem for cerebral perfusion. Key modernisations for \texttt{dolfinx‑0.9} are highlighted below.

\subsubsection{Ghost Mode Configuration}

In legacy \texttt{dolfin}, ghost‐mode was set via parameters:

\begin{lstlisting}[language=Python,caption=Legacy ghost‐mode setting]
# ghost mode options: 'none', 'shared_facet', 'shared_vertex'
parameters['ghost_mode'] = 'none'
\end{lstlisting}

In \texttt{dolfinx}, ghost‐mode is configured at mesh creation:

\begin{lstlisting}[language=Python,caption=FEniCSx ghost‐mode setting~\cite{dolfinxPyDocs}]
import dolfinx.mesh
# Disable ghosting entirely
mesh_obj = dolfinx.mesh.create_mesh(..., ghost_mode=dolfinx.mesh.GhostMode.NONE)
\end{lstlisting}

\subsubsection{Logging Configuration}

Legacy code silenced output via:

\begin{lstlisting}[language=Python,caption=Legacy log suppression]
from dolfin import *
set_log_level(50)
\end{lstlisting}

The modern script uses the new logging API:

\begin{lstlisting}[language=Python,caption=FEniCSx log suppression~\cite{dolfinxPyDocs}]
from dolfinx.log import set_log_level, LogLevel
set_log_level(LogLevel.WARNING)
\end{lstlisting}

\subsubsection{Argument Parsing and Configuration}

The legacy solver parsed its YAML config file as follows:

\begin{lstlisting}[language=Python,caption=Legacy argument parsing]
parser = argparse.ArgumentParser(...)
parser.add_argument("--config_file", ...)
config_file = parser.parse_args().config_file
configs = IO_fcts.basic_flow_config_reader_yml(config_file, parser)
\end{lstlisting}

In the modern version, the same pattern applies but with streamlined defaults and error handling:

\begin{lstlisting}[language=Python,caption=FEniCSx argument parsing]
parser = argparse.ArgumentParser(...)
parser.add_argument("--config_file", type=str,
                    default="./config_basic_flow_solver.yaml")
args = parser.parse_args()
configs = IO_fcts.basic_flow_config_reader_yml(args.config_file, parser)
\end{lstlisting}

\subsubsection{Result Output}

Legacy output of solution and timing used checkpoint writes and manual \texttt{sys.stdout} redirection:

\begin{lstlisting}[language=Python,caption=Legacy result saving]
with XDMFFile(res_fldr+'K1_form.xdmf') as f:
    f.write_checkpoint(K1,"K1_form",0,...)
# timing saved by redirecting sys.stdout to a log file
old = sys.stdout
sys.stdout = open(res_fldr+"time.log","w")
print("Total time:", total)
sys.stdout = old
\end{lstlisting}

The FEniCSx script uses the new I/O API and context managers:

\begin{lstlisting}[language=Python,caption=FEniCSx result saving~\cite{dolfinxPyDocs}]
from dolfinx.io import XDMFFile
with XDMFFile(mesh.comm, res_fldr+"K1_form.xdmf","w",
              encoding=XDMFFile.Encoding.HDF5) as xdmf:
    xdmf.write_mesh(mesh)
    xdmf.write_function(K1, 0)

# timing logged via standard file I/O
with open(res_fldr+"time_info.log","w") as logf:
    print("Total time:", total, file=logf)
\end{lstlisting}

---

Each of these updates preserves the original computational pipeline—reading configuration, allocating function spaces, assembling and solving the variational problem, and writing results—while adopting \texttt{dolfinx-0.9}'s explicit, MPI‐aware API.

\subsubsection{Changes in Function‐Call Patterns}

In migrating the \texttt{basic\_flow\_solver.py} from legacy \texttt{dolfin} to \texttt{dolfinx-0.9}, we retained the overall workflow but refactored many function signatures to expose parameters explicitly and improve readability.

\paragraph{1. Mesh I/O and Configuration Reader}

\textbf{Legacy:}
\begin{lstlisting}[language=Python,caption=Legacy configuration and mesh I/O]
configs = IO_fcts.basic_flow_config_reader_yml(config_file, parser)
mesh, subdomains, boundaries = IO_fcts.mesh_reader(configs['input']['mesh_file'])
\end{lstlisting}

\textbf{Modern \texttt{dolfinx-0.9}}:
\begin{lstlisting}[language=Python,caption=Modern configuration and mesh I/O]
configs = IO_fcts.basic_flow_config_reader_yml(args.config_file, parser)
mesh, subdomains, boundaries = IO_fcts.mesh_reader(
    configs['input']['mesh_file']
)
\end{lstlisting}

\emph{Note:} The new reader validates additional fields (e.g.\ \texttt{healthy}, \texttt{occl\_ID}) but the call structure remains unchanged.

\paragraph{2. Function‐Space Allocation}

\textbf{Legacy:}
\begin{lstlisting}[language=Python,caption=Legacy function‐space allocation]
Vp, Vvel, v_1, v_2, v_3, p, p1, p2, p3, K1_space, K2_space = \
    fe_mod.alloc_fct_spaces(mesh, fe_degree, model_type, vel_order)
\end{lstlisting}

\textbf{Modern \texttt{dolfinx-0.9}}:
\begin{lstlisting}[language=Python,caption=Modern function‐space allocation]
Vp, Vvel, v_1, v_2, v_3, p, p1, p2, p3, K1_space, K2_space = \
    fe_mod.alloc_fct_spaces(
        mesh,
        configs['simulation']['fe_degr'],
        model_type=compartmental_model,
        vel_order=velocity_order
    )
\end{lstlisting}

\emph{Change:} Keyword arguments (\texttt{model\_type}, \texttt{vel\_order}) clarify intent and improve extensibility.

\paragraph{3. Solver Setup}

\textbf{Legacy:}
\begin{lstlisting}[language=Python,caption=Legacy solver setup]
LHS, RHS, sigma1, sigma2, sigma3, BCs = \
    fe_mod.set_up_fe_solver2(
        mesh, subdomains, boundaries,
        Vp, v_1, v_2, v_3, p, p1, p2, p3,
        K1, K2, K3, beta12, beta23,
        p_arterial, p_venous,
        configs['input']['read_inlet_boundary'],
        configs['input']['inlet_boundary_file'],
        configs['input']['inlet_BC_type']
    )
\end{lstlisting}

\textbf{Modern \texttt{dolfinx-0.9}}:
\begin{lstlisting}[language=Python,caption=Modern solver setup]
LHS, RHS, sigma1, sigma2, sigma3, BCs = \
    fe_mod.set_up_fe_solver2(
        mesh_obj, subdomains, boundaries,
        Vp, v_1, v_2, v_3, p, p1, p2, p3,
        K1, K2, K3, beta12, beta23,
        p_arterial, p_venous,
        configs['input']['read_inlet_boundary'],
        configs['input']['inlet_boundary_file'],
        configs['input']['inlet_BC_type'],
        model_type=compartmental_model
    )
\end{lstlisting}

\emph{Change:} The explicit \texttt{model\_type} keyword avoids hidden defaults and documents the solver’s configuration.

\paragraph{4. Linear Solver Call}

\textbf{Legacy:}
\begin{lstlisting}[language=Python,caption=Legacy linear solver invocation]
p = fe_mod.solve_lin_sys(
    Vp, LHS, RHS, BCs,
    lin_solver, precond, rtol, mon_conv, init_sol
)
\end{lstlisting}

\textbf{Modern \texttt{dolfinx-0.9}}:
\begin{lstlisting}[language=Python,caption=Modern linear solver invocation]
p = fe_mod.solve_lin_sys(
    Vp, LHS, RHS, BCs,
    lin_solver, precond,
    rtol, mon_conv, init_sol,
    inlet_BC_type,
    model_type=compartmental_model
)
\end{lstlisting}

\emph{Change:} Passing \texttt{inlet\_BC\_type} allows the solver to adjust PETSc options based on boundary‐condition type; \texttt{model\_type} continues to guide internal logic.

\vspace{0.5em}
Overall, function calls now uniformly use keyword arguments for any non‐trivial option. This enhances maintainability, self‐documentation, and flexibility for batch experiments or future extensions.


\subsection{Boundary Condition Generator Modernisation}

In the legacy \texttt{FEniCS} implementation, boundary condition assignment was handled via a script called \texttt{BC\_creator.py}, which assigned pressures and volumetric flow rates to surface regions of the cerebral mesh. These values were distributed based on surface area and optionally modified to reflect artery occlusions. Execution relied heavily on command-line arguments passed to \texttt{argparse}, requiring the user to explicitly set flags for occlusions and mesh paths:

\begin{lstlisting}[language=bash, caption=Legacy boundary condition invocation]
python3 BC_creator.py --occluded --occl_ID 25 --mesh_file path/to/mesh.xdmf
\end{lstlisting}

The new \texttt{FEniCSx}-based implementation shifts to a configuration-driven model, removing the need for command-line specification of occlusions. Instead, simulation state and pathology are encoded directly in a YAML config file:

\begin{lstlisting}[language=yaml, caption=Occlusion config snippet]
input:
  healthy: False
  occl_ID: [25]
  inlet_BC_type: MIXED
\end{lstlisting}

This enables a significantly cleaner and more modular design. Major differences in the modern implementation include:

\begin{itemize}
    \item \textbf{Centralised Configuration:} All logic related to occlusion state, output folder, and mesh location is controlled from a single YAML file, streamlining batch execution and improving reproducibility.
    \item \textbf{Surface-Area Scaling:} The total volumetric inflow (\texttt{Q\_brain}) is distributed across boundary regions proportionally to their surface area, consistent with the legacy implementation.
    \item \textbf{Arterial Mapping Logic:} Artery IDs are mapped via internal dictionaries to groups of cluster labels. This replaces the external CSV-based \texttt{boundary\_mapper} logic and ensures compatibility across different mesh resolutions.
    \item \textbf{Mixed Boundary Condition Support:} The new script allows specification of \texttt{DBC}, \texttt{NBC}, or \texttt{MIXED} inlet conditions. For mixed cases, occluded arteries automatically receive Neumann BCs while others use Dirichlet conditions.
    \item \textbf{MPI-safe Output:} Output is written only by rank 0, ensuring compatibility with parallel runs.
\end{itemize}

\begin{lstlisting}[language=Python, caption=Modern artery-mapped boundary matrix row]
[label, Q_i, P_i, artery_ID, flag]
\end{lstlisting}

This updated implementation better aligns with HPC workflows, supports automated parameter sweeps, and reduces the potential for user error. It also eliminates several deprecated constructs in FEniCS such as direct access to boundary labels through subdomain functions, replacing them with topologically defined boundary arrays compatible with \texttt{dolfinx.mesh.MeshTags}.

\subsubsection{Arterial Mapping Logic}

In the legacy implementation, the association between surface region labels and their corresponding feeding arteries was handled via an external CSV file called \texttt{boundary\_mapper.csv}. This file contained a manually prepared mapping between mesh surface labels and major cerebral arteries. The code parsed this file and constructed a lookup table used during boundary condition assignment:

\begin{lstlisting}[language=Python, caption=Legacy mapping from CSV file]
boundary_mapper = np.loadtxt('boundary_mapper.csv', skiprows=1, delimiter=',')
boundary_map = np.zeros(len(boundary_values))
for i in list(boundary_values):
    boundary_map[idx] = int(boundary_mapper[np.argwhere(boundary_mapper[:,1]==i)[0],0])
\end{lstlisting}

While functional, this approach introduced additional dependencies, increased fragility across mesh versions, and complicated automation.

\paragraph{Modern Approach: Hardcoded Arterial Groups}

The \texttt{FEniCSx} implementation removes this external dependency by introducing an internally defined mapping between cluster IDs and cerebral arteries using structured Python dictionaries:

\begin{lstlisting}[language=Python, caption=Modern arterial group assignment]
artery_groups = {
    24: list(range(20, 32)),   # Right ACA
    25: list(range(32, 52)),   # Right MCA
    22: list(range(52, 80)),   # Left MCA
    21: list(range(80, 100)),  # Left ACA
    26: list(range(89, 100)),  # Right PCA
    23: list(range(100, 112)), # Left PCA
    4: list(range(112, 118)),  # Brainstem or circle of Willis
    30: list(range(118, 130))  # Basilar or other midline
}
\end{lstlisting}

Each boundary label is iteratively checked against this mapping to determine its corresponding artery:

\begin{lstlisting}[language=Python]
for art_id, label_list in artery_groups.items():
    if label in label_list:
        artery = art_id
\end{lstlisting}

This redesign offers several advantages:
\begin{itemize}
    \item \textbf{Mesh Version Independence:} Removes the need to maintain external CSV files for each new mesh discretisation.
    \item \textbf{Clarity and Control:} Makes arterial mapping transparent and easy to update or extend within the code itself.
    \item \textbf{Robustness:} Prevents issues arising from corrupted or inconsistent mapping files.
    \item \textbf{Streamlined Automation:} Batch runs across multiple occlusion configurations do not require separate file dependencies.
\end{itemize}

Overall, the switch to internally defined artery groups significantly improves the maintainability and portability of the codebase while preserving the anatomical fidelity of the boundary condition application.


\subsection{IO\_fcts}

The following functions are listed in the order in which they appear within the perfusion model run-through. Some of these functions are shared by the scripts in the \texttt{perfusion} folder.

\subsubsection*{perm\_init\_config\_reader\_yml}

\textbf{Note:} No changes were made to the code itself, however extensive documentation was added in the DolfinX version for usability.

\subsubsection*{mesh\_reader}

\paragraph{Reading mesh}
\begin{itemize}
  \item \textbf{Dolfin Version}:
\begin{lstlisting}[language=Python]
mesh = Mesh()
with XDMFFile(comm, mesh_file) as myfile:
    myfile.read(mesh)
\end{lstlisting}

  \item \textbf{DolfinX Version}:
\begin{lstlisting}[language=Python]
with XDMFFile(comm, mesh_file, "r") as xdmf_file:
    mesh = xdmf_file.read_mesh(name="mesh")
\end{lstlisting}
\end{itemize}

\paragraph{Key Differences}
\begin{itemize}
  \item \textbf{Improvement:} The DolfinX version makes the mesh reading process more explicit and aligns better with the DolfinX API.
\end{itemize}

\paragraph{Connectivity Creation}
\begin{itemize}
  \item \textbf{Dolfin Version:} No explicit connectivity creation.
  \item \textbf{DolfinX Version:}
\begin{lstlisting}[language=Python]
mesh.topology.create_connectivity(mesh.topology.dim, mesh.topology.dim - 1)
mesh.topology.create_connectivity(mesh.topology.dim - 1, mesh.topology.dim)
\end{lstlisting}
\item \textbf{Improvement:} Explicit creation is beneficial for boundary conditions and integration on facets.
\end{itemize}

\paragraph{Mesh Validation}
\begin{itemize}
  \item \textbf{DolfinX Version:}
\begin{lstlisting}[language=Python]
if not isinstance(mesh, dolfinx.mesh.Mesh):
    raise ValueError(f"Failed to load a valid mesh from {mesh_file}. Check the file format.")
\end{lstlisting}
\item \textbf{Improvement:} Ensures proper mesh format and prevents downstream errors.
\end{itemize}

\paragraph{Subdomains and Boundaries}
\begin{itemize}
  \item \textbf{Dolfin Version:}
\begin{lstlisting}[language=Python]
subdomains = MeshFunction("size_t", mesh, 3)
with XDMFFile(comm, mesh_file[:-5]+'_physical_region.xdmf') as myfile:
    myfile.read(subdomains)
boundaries = MeshFunction("size_t", mesh, 2)
with XDMFFile(comm, mesh_file[:-5]+'_facet_region.xdmf') as myfile:
    myfile.read(boundaries)
\end{lstlisting}

  \item \textbf{DolfinX Version:}
\begin{lstlisting}[language=Python]
physical_region_file = mesh_file[:-5] + '_physical_region.xdmf'
with XDMFFile(comm, physical_region_file, "r") as xdmf_file:
    subdomains = xdmf_file.read_meshtags(mesh, name="mesh")

facet_region_file = mesh_file[:-5] + '_facet_region.xdmf'
with XDMFFile(comm, facet_region_file, "r") as xdmf_file:
    boundaries = xdmf_file.read_meshtags(mesh, name="mesh")
\end{lstlisting}

  \item \textbf{Improvement:} More robust handling with \texttt{read\_meshtags}.
  \item \textbf{New Complexity:}
\begin{lstlisting}[language=Python]
try:
    with XDMFFile(comm, physical_region_file, "r") as xdmf_file:
        subdomains = xdmf_file.read_meshtags(mesh, name="mesh")
except Exception as e:
    subdomains = None
\end{lstlisting}
\end{itemize}

\paragraph{Communication Barrier}
\begin{itemize}
  \item \textbf{DolfinX Version:}
\begin{lstlisting}[language=Python]
comm.Barrier()
\end{lstlisting}
  \item \textbf{Improvement:} Synchronizes processes in parallel simulations.
\end{itemize}

\paragraph{print0}
\begin{itemize}
  \item \textbf{Dolfin Version:} Manually wrapped print statements in \texttt{rank == 0}.
  \item \textbf{DolfinX Version:} Uses a reusable function:
\begin{lstlisting}[language=Python]
from mpi4py import MPI

def print0(*args, **kwargs):
    if MPI.COMM_WORLD.rank == 0:
        print(*args, **kwargs)
\end{lstlisting}
  \item \textbf{Improvement:} Enhances readability and avoids repeated code.
\end{itemize}

\subsubsection*{basic\_flow\_config\_reader\_yml}

\textbf{Note:} No changes were made to the code itself, however extensive documentation was added in the DolfinX version for usability.

\subsubsection*{initialise\_permeabilities}

In the transition from legacy Dolfin to DolfinX, the \texttt{initialise\_permeabilities} function was substantially revised due to the removal of high-level utilities like \texttt{read\_checkpoint}.

\paragraph{Main Differences}
\begin{itemize}
  \item \textbf{Dolfin Version:}
\begin{lstlisting}[language=Python]
K = Function(V)
K = read_checkpoint(xdmf_file, "K", V)
\end{lstlisting}

  \item \textbf{DolfinX Version:} Introduces two new functions:

\texttt{find\_dataset\_key}
\begin{lstlisting}[language=Python]
def find_dataset_key(f, key):
    if key in f:
        return key
    for group_key in f:
        if isinstance(f[group_key], h5py.Group):
            result = find_dataset_key(f[group_key], key)
            if result is not None:
                return f"{group_key}/{result}"
    return None
\end{lstlisting}

\texttt{read\_function\_from\_h5}
\begin{lstlisting}[language=Python]
def read_function_from_h5(K, filename, dataset_name):
    with h5py.File(filename, "r", driver="mpio", comm=K.function_space.mesh.comm) as f:
        dataset_path = find_dataset_key(f, dataset_name)
        full_data = f[dataset_path][:]
    dofmap = K.function_space.dofmap
    local_size = len(dofmap.list.array)
    local_data = full_data[:local_size]
    K.x.array[:] = local_data
\end{lstlisting}
\end{itemize}

\paragraph{Key Differences}
\begin{itemize}
  \item \textbf{Loss of Convenience:} \texttt{read\_checkpoint} and \texttt{read\_function} are no longer supported.
  \item \textbf{Increased Complexity:} Manual data loading, MPI communication, and assignment are required.
  \item \textbf{Greater Flexibility:} Full control over HDF5 layout and MPI data scattering.
  \item \textbf{Improved Debugging:} Modular print statements using \texttt{print0} for tracing.
\end{itemize}

\textbf{Conclusion:} The DolfinX approach trades off simplicity for flexibility and scalability, suitable for high-performance parallel computing environments.


\subsection{suppl\_fcts}

The following functions are listed in the order in which they appear within the perfusion model run-through. Some of these functions are shared across scripts in the \texttt{perfusion} folder.

\subsubsection{comp\_vessel\_orientation}

The function \texttt{comp\_vessel\_orientation} computes vessel orientation based on a permeability field derived from boundary conditions. Both DolfinX and legacy implementations share the same objective but differ in execution, as outlined below:
\paragraph{Function Space Definition}
\begin{lstlisting}[language=Python, caption={Legacy Function Space}]
V = fem.FunctionSpace(mesh, "CG", 1)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={DolfinX Function Space}]
V = fem.FunctionSpace(mesh, ("CG", 1))
\end{lstlisting}


\paragraph{Boundary Conditions Setup}
\begin{lstlisting}[language=Python, caption={Legacy DirichletBC}]
bc = DirichletBC(V, Constant(0.0), boundary_markers, 1)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={DolfinX DirichletBC}]
fdim = mesh.topology.dim - 1
boundary_facets = mesh.topology.find_entities(fdim, marker)
dofs = fem.locate_dofs_topological(V, fdim, boundary_facets)
bc = fem.dirichletbc(PETSc.ScalarType(0), dofs, V)
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} Boundary condition assignment now uses low-level DOF maps rather than string expressions.
    \item \textbf{Benefit:} Enables mesh-independent and scalable boundary condition definitions suitable for parallel environments.
    \item \textbf{Negative:} Requires explicit geometry-based functions or labels, increasing code complexity.
\end{itemize}

\paragraph{Solver Setup and Solution}
\begin{lstlisting}[language=Python, caption={Legacy Solve}]
solve(a == L, u, bc)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={DolfinX Solve}]
problem = fem.petsc.LinearProblem(a, L, bcs=[bc])
u = problem.solve()
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} DolfinX replaces global `solve` with a `LinearProblem` object.
    \item \textbf{Benefit:} Better encapsulation of linear solver settings and increased solver flexibility.
    \item \textbf{Negative:} More verbose setup and the need to separately handle forward scatter of solutions.
\end{itemize}

\paragraph{Gradient Projection and Normalization}
\begin{lstlisting}[language=Python, caption={DolfinX Gradient Projection}]
grad_u = ufl.grad(u)
projected_grad = fem.Function(W)
fem.project(grad_u, W, projected_grad)
normed_grad = projected_grad / ufl.sqrt(ufl.inner(projected_grad, projected_grad))
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} Manual gradient computation and projection are more explicit in DolfinX.
    \item \textbf{Benefit:} Allows modular projections into various spaces and better control over function norms.
    \item \textbf{Negative:} Requires additional projection logic and normalization steps.
\end{itemize}

\paragraph{Parallelization and I/O Handling}
\begin{lstlisting}[language=Python, caption={DolfinX Parallel Output}]
with io.XDMFFile(MPI.COMM_WORLD, filename, "w") as xdmf:
    xdmf.write_mesh(mesh)
    xdmf.write_function(function)
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} DolfinX enforces explicit MPI rank management and new XDMF I/O patterns.
    \item \textbf{Benefit:} Ensures correct file handling in distributed runs and compatibility with parallel file formats.
    \item \textbf{Negative:} Requires conditional `rank == 0` blocks and manual handling of XDMF I/O life cycle.
\end{itemize}

\subsubsection{region\_label\_assembler}

The following examples demonstrate the differences between the Dolfin (legacy) and DolfinX implementations of the \texttt{region\_label\_assembler} routine, which is used to gather and broadcast unique region tags across MPI ranks. Key updates include syntax modernization, explicit data casting, and MPI semantics using \texttt{MPI.COMM\_WORLD}.

\paragraph{Key Differences and Their Implications}~

\paragraph{MeshTag Access Differences}~

\textbf{DolfinX:}
\begin{lstlisting}
[language=Python, caption=Accessing region labels using DolfinX's MeshTags.values]
region_labels = region.values
\end{lstlisting}
\textbf{Legacy Dolfin:}
\begin{lstlisting}[language=Python, caption=Accessing region labels using Dolfin's MeshFunction.array()]
region_labels = region.array()
\end{lstlisting}

\paragraph{ Explicit Casting to \texttt{int64} and Use of Modulo}~

\textbf{DolfinX:}
\begin{lstlisting}[language=Python, caption=Forcing int64 casting and positive int32 mapping in DolfinX]
region_labels = np.array(region_labels, dtype=np.int64)
region_labels = (region_labels % (2**31)).astype(np.int64)
\end{lstlisting}
\textbf{Legacy Dolfin:}
\begin{lstlisting}[language=Python, caption=Absence of explicit casting in Dolfin (legacy) implementation]
region_labels = region.array()
\end{lstlisting}


\paragraph{MPI Interface Differences}~
\textbf{DolfinX:}
\begin{lstlisting}[language=Python, caption=Modernized MPI initialization using MPI.COMM\_WORLD in DolfinX]
comm = MPI.COMM_WORLD
\end{lstlisting}
\textbf{Legacy Dolfin:}
\begin{lstlisting}[language=Python, caption=Legacy MPI initialization using MPI.comm\_world in Dolfin]
comm = MPI.comm_world
\end{lstlisting}


\paragraph{Scalar Extraction for Label Count}~


\textbf{DolfinX:}
\begin{lstlisting}[language=Python, caption=Explicit scalar casting and type annotations in DolfinX]
n_labels = int(n_labels[0])
\end{lstlisting}
\textbf{Legacy Dolfin:}
\begin{lstlisting}[language=Python, caption=Implicit typing and return conversion in Dolfin (legacy)]
n_labels = n_labels[0]
\end{lstlisting}

\subsubsection{perm\_tens\_comp}

\begin{lstlisting}[language=Python, caption=Legacy Dolfin version]
K1 = Function(K_space)    
K1_array = K1.vector().get_local()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=DolfinX version]
K1 = fem.Function(K_space)
K1_arr = np.zeros(K1.x.array.shape, dtype=K1.x.array.dtype)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Legacy Dolfin loop]
for i in range(mesh.num_cells()):
    e1 = e_loc_array[i*3:i*3+3]
    ...
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=DolfinX loop with MPI-awareness]
for local_cell in range(start, end):
    off = local_cell - start
    e1 = e_loc_arr[off*3:(off+1)*3]
    ...
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} Legacy loops iterate over all mesh cells, including non-local ones.
    \item \textbf{Benefit:} DolfinX version respects MPI domain decomposition via `IndexMap`.
    \item \textbf{Downside:} Must explicitly compute local index offset and manage parallel ranges.
\end{itemize}

\begin{lstlisting}[language=Python, caption=Legacy result assignment]
K1_array[i*9:i*9+9] = K1_loc
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=DolfinX result assignment]
K1_arr[start_idx:end_idx] = flat
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} Naming and index conventions reflect MPI partitioning of cell ownership.
    \item \textbf{Benefit:} Clear and explicit mapping between local cell index and vector slice.
    \item \textbf{Downside:} Slightly more manual bookkeeping required compared to legacy version.
\end{itemize}

\begin{lstlisting}[language=Python, caption=Legacy finalize]
K1.vector().set_local(K1_array)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=DolfinX finalize]
K1.x.array[:] = K1_arr
K1.x.scatter_forward()
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} Legacy method for setting local values is replaced by `.x.array` assignment and scatter.
    \item \textbf{Benefit:} Explicit control over memory layout and MPI synchronization.
    \item \textbf{Downside:} Requires additional call to `scatter\_forward()` to finalize.
\end{itemize}

\subsubsection{comp\_transf\_mat}

\begin{lstlisting}[language=Python, caption=Legacy version]
v = np.cross(e0,e1)
s = np.linalg.norm(v) # sine
c = np.dot(e0,e1) # cosine
...
T = c*I + s*ux + (1-c)*np.tensordot(u,u,axes=0)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=DolfinX version]
e0 = e0 / np.linalg.norm(e0)
e1 = e1 / np.linalg.norm(e1)
v = np.cross(e0, e1)
...
T = c * np.identity(3) + s * ux + (1 - c) * np.outer(u, u)
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} DolfinX version includes normalization and shape validation for input vectors.
    \item \textbf{Benefit:} Robustness against numerical errors and degenerate vectors (e.g., zero vectors or parallel).
    \item \textbf{Downside:} More code and checks, slightly reduced performance in tight loops.
\end{itemize}

\begin{lstlisting}[language=Python, caption=Legacy edge case handling (commented out)]
# rot_vec = v/s*np.arcsin(s)
# transformation_matrix = R.from_rotvec(rot_vec)
# T = transformation_matrix.as_dcm()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=DolfinX robust edge case handling]
if s == 0:
    if c > 0:
        return np.identity(3)
    else:
        ortho = np.array([1.0, 0.0, 0.0])
        ...
        return -np.identity(3) + 2 * np.outer(axis, axis)
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason:} Explicit check for parallel and anti-parallel vectors, which could cause division by zero.
    \item \textbf{Benefit:} Graceful handling of edge cases without relying on external libraries like \texttt{scipy.spatial.transform}.
    \item \textbf{Downside:} Extra logic required to define an orthogonal rotation axis manually.
\end{itemize}



\subsubsection{scale\_coupling\_coefficients}

\subsubsection*{scale\_coupling\_coefficients}

\begin{lstlisting}[language=Python]
# DolfinX version
loc1 = subdomains.where_equal(11)  # white matter cell indices
loc2 = subdomains.where_equal(12)  # gray matter cell indices

beta12_array = beta12.vector().get_local()
beta23_array = beta23.vector().get_local()

beta12.vector().set_local(beta12_array)
beta23.vector().set_local(beta23_array)
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Legacy version
loc1 = subdomains.indices[np.where(subdomains.values == 11)]  # white matter cell indices
loc2 = subdomains.indices[np.where(subdomains.values == 12)]  # gray matter cell indices

beta12_array = beta12.x.array
beta23_array = beta23.x.array

beta12.x.array[:] = beta12_array
beta23.x.array[:] = beta23_array
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason for change:} The legacy version uses \texttt{subdomains.indices} and \texttt{subdomains.values} to extract indices, while the DolfinX version uses \texttt{subdomains.where\_equal()} to extract indices for white matter (11) and gray matter (12).
\end{itemize}

\begin{itemize}
    \item \textbf{Reason for change:} The legacy version accesses the local array using \texttt{beta12.x.array}, while the DolfinX version uses \texttt{beta12.vector().get\_local()}.

\end{itemize}

\begin{itemize}
    \item \textbf{Reason for change:} The DolfinX version uses \texttt{beta12.vector().set\_local(beta12\_array)} to set the array, while the legacy version uses \texttt{beta12.x.array[:] = beta12\_array}.

\end{itemize}


\begin{lstlisting}[language=Python]
# DolfinX version
beta12_array[loc2] = beta12gm
beta12_array[loc1] = beta12gm / gmowm_beta_rat
beta23_array[loc2] = beta23gm
beta23_array[loc1] = beta23gm / gmowm_beta_rat
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Legacy version
beta12_array[loc2] = beta12gm
beta12_array[loc1] = beta12gm / gmowm_beta_rat
beta23_array[loc2] = beta23gm
beta23_array[loc1] = beta23gm / gmowm_beta_rat
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason for change:} The assignment of values to \texttt{beta12\_array} and \texttt{beta23\_array} is identical between DolfinX and legacy versions, so no change was made here.
\end{itemize}

\begin{lstlisting}[language=Python]
# DolfinX version
beta12.vector().set_local(beta12_array)
beta23.vector().set_local(beta23_array)
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Legacy version
beta12.x.array[:] = beta12_array
beta23.x.array[:] = beta23_array
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason for change:} In DolfinX, the vectors are updated with \texttt{beta12.vector().set\_local(beta12\_array)} while in legacy Dolfin, \texttt{beta12.x.array[:]} is used.
\end{itemize}

\begin{lstlisting}[language=Python]
# DolfinX version
return beta12, beta23
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Legacy version
return beta12, beta23
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason for change:} The return statement is identical in both versions, returning the same functions \texttt{beta12} and \texttt{beta23}.
\end{itemize}

\begin{lstlisting}[language=Python]
# Legacy version
print("Debug: loc1 (WM) indices:", loc1)
print("Debug: loc2 (GM) indices:", loc2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
# DolfinX version
# Debugging prints have been removed in the DolfinX version.
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason for change:} The legacy version includes debugging print statements for indices, while the DolfinX version omits these prints.
    \item \textbf{Benefit:} The DolfinX version focuses on cleaner code without unnecessary debug prints in production code.
    \item \textbf{Negative:} Legacy users might find it harder to debug without these explicit prints.
\end{itemize}

\begin{lstlisting}[language=Python]
# Legacy version
assert np.allclose(beta12_array[loc2], beta12gm, atol=1e-8), "Mismatch in beta12 for GM"
assert np.allclose(beta12_array[loc1], beta12gm / gmowm_beta_rat, atol=1e-8), "Mismatch in beta12 for WM"
assert np.allclose(beta23_array[loc2], beta23gm, atol=1e-8), "Mismatch in beta23 for GM"
assert np.allclose(beta23_array[loc1], beta23gm / gmowm_beta_rat, atol=1e-8), "Mismatch in beta23 for WM"
\end{lstlisting}

\begin{lstlisting}[language=Python]
# DolfinX version
# The assertion logic is the same, but the DolfinX version doesn't have this explicit check in the code as the assumption is handled by internal methods.
\end{lstlisting}

\begin{itemize}
    \item \textbf{Reason for change:} The legacy version has explicit assertions to check for mismatches in the values assigned to \texttt{beta12\_array} and \texttt{beta23\_array}, while DolfinX assumes this internally with the functions.
\end{itemize}


\subsubsection{scale\_permeabilities}


\begin{lstlisting}[language=Python, caption={DolfinX: Extracting WM and GM indices via explicit Tag arrays}]
loc1 = subdomains.indices[np.where(subdomains.values == 11)]  # white matter
loc2 = subdomains.indices[np.where(subdomains.values == 12)]  # gray  matter
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Legacy: Using \texttt{where\_equal()} to extract indices}]
loc1 = subdomains.where_equal(11)  # white matter
loc2 = subdomains.where_equal(12)  # gray  matter
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} DolfinX uses raw \texttt{indices} and \texttt{values} arrays for mesh tags instead of the legacy helper method.
\end{itemize}

\begin{lstlisting}[language=Python, caption={DolfinX: Accessing Function data via \texttt{x.array}}]
K1_array = K1.x.array
K2_array = K2.x.array
K3_array = K3.x.array
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Legacy: Accessing Function data via \texttt{.vector().get\_local()}}]
K1_array = K1.vector().get_local()
K2_array = K2.vector().get_local()
K3_array = K3.vector().get_local()
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} DolfinX unified the local data interface under \texttt{.x.array} instead of separate PETSc vectors.
\end{itemize}


\begin{lstlisting}[language=Python, caption={DolfinX: Writing back and scattering ghost values}]
K1.x.array[:] = K1_array
K2.x.array[:] = K2_array
K3.x.array[:] = K3_array
K1.x.scatter_forward()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Legacy: Setting local values via PETSc vector API}]
K1.vector().set_local(K1_array)
K2.vector().set_local(K2_array)
K3.vector().set_local(K3_array)
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} DolfinX merges assignment and ghost communication under the \texttt{.x} interface.
\end{itemize}


\subsubsection{compute\_my\_variables}

\begin{lstlisting}[language=Python, caption={Legacy: \texttt{save\_data} via conditional kwarg check}]
if 'save_data' in kwarg:
    save_data = kwarg.get('save_data')
else:
    save_data = True
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: default via \texttt{dict.get}}]
save_data = kwarg.get("save_data", True)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Legacy: Inline L²‐projection for perfusion}]
if 'perfusion' in out_vars:
    myResults['perfusion'] = project(beta12 * (p1-p2), K2_space,
                                     solver_type='bicgstab',
                                     preconditioner_type='petsc_amg')
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Using \texttt{project\_expression} helper}]
if "perfusion" in out_vars:
    myResults["perfusion"] = project_expression(beta12*(p1 - p2), K2_space)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Legacy: Velocity via \texttt{project}}]
if 'vel1' in out_vars:
    myResults['vel1'] = project(-K1*grad(p1), Vvel,
                                solver_type='bicgstab',
                                preconditioner_type='petsc_amg')
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Tensor–vector interpolation via \texttt{interpolate\_tensor\_expression}}]
if "vel1" in out_vars:
    myResults["vel1"] = interpolate_tensor_expression(K1, -ufl.grad(p1), Vvel)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={DolfinX: Scalar‐valued interpolation via \texttt{interpolate\_expression}}]
if "vel2" in out_vars:
    myResults["vel2"] = interpolate_expression(-K2*ufl.grad(p2), Vvel)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Legacy: Blocking \texttt{XDMFFile.write\_checkpoint}}]
with XDMFFile(res_fldr+var+'.xdmf') as myfile:
    myfile.write_checkpoint(myResults[var], var, 0,
                            XDMFFile.Encoding.HDF5, False)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Parallel‐safe saving with \texttt{interpolate\_to\_mesh\_order}}]
os.environ["HDF5_USE_FILE_LOCKING"] = "FALSE"
for var in out_vars:
    # remove old files...
    var_to_save = interpolate_to_mesh_order(myResults[var], mesh_order=1)
    with XDMFFile(comm, f"{res_fldr}{var}.xdmf", "w") as xdmf:
        xdmf.write_mesh(get_mesh_from_var(var_to_save))
        xdmf.write_function(
            scaled if var=="perfusion" else var_to_save
        )
\end{lstlisting}

\paragraph{Key Differences}
\begin{itemize}
  \item \textbf{Helper-based Projections:} All inline \texttt{project(...)} calls are replaced by \texttt{project\_expression} and \texttt{project\_tensor\_expression}, centralizing solver setup.
  \item \textbf{Nodal Interpolation for Velocities:} Velocity fields now use \texttt{interpolate\_expression} or \texttt{interpolate\_tensor\_expression}, leveraging C++‐level nodal evaluation rather than global projection.
  \item \textbf{Uniform Output Discretization:} \texttt{interpolate\_to\_mesh\_order} ensures every saved Function lives in a consistent CG(mesh\_order) space.
  \item \textbf{Simplified Defaults:} \texttt{save\_data} and other flags use \texttt{dict.get(..., default)} rather than manual branching.
  \item \textbf{Parallel‐safe I/O and Feedback:} File locking is disabled, old files are cleaned up, and \texttt{tqdm} progress bars report saving status.
  \item \textbf{Rank-0 Logging:} A small \texttt{print0} wrapper abstracts the \texttt{if rank==0} check for all diagnostic messages.
\end{itemize}

The DolfinX version of \texttt{compute\_my\_variables} replaces several inline operations with dedicated helper functions. Each helper is described in its own subsubsection below, alongside the legacy code it supersedes.

\subsubsection*{project\_expression}
\begin{lstlisting}[language=Python]
def project_expression(f_expr, V, petsc_options={"ksp_type":"bcgs","pc_type":"hypre"}):
    """
    Generic L²‐projection of the UFL expression f_expr onto V.
    """
    u = ufl.TrialFunction(V)
    v = ufl.TestFunction(V)
    a = ufl.inner(u, v) * ufl.dx
    L = ufl.inner(f_expr, v) * ufl.dx
    problem = LinearProblem(a, L, bcs=[], petsc_options=petsc_options)
    return problem.solve()
\end{lstlisting}

\textbf{Replaces:}
\begin{lstlisting}[language=Python]
# Inline projection in legacy code
myResults['perfusion'] = project(beta12 * (p1-p2), K2_space,
                                 solver_type='bicgstab',
                                 preconditioner_type='petsc_amg')
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Factor out repeated L²‐projection setup.
  \item \textbf{Benefit:} Centralises solver options; DRYs up code.
  \item \textbf{Negative:} Introduces an extra layer of indirection.
\end{itemize}

\subsubsection*{project\_tensor\_expression}
\begin{lstlisting}[language=Python]
def project_tensor_expression(K, g, V, petsc_options={"ksp_type":"bcgs","pc_type":"hypre"}):
    """
    L²‐project the tensor‐vector product K⋅g onto V.
    """
    mesh_dim = V.mesh.topology.dim
    if hasattr(K, "ufl_shape") and K.ufl_shape == (mesh_dim, mesh_dim):
        K_tensor = K
    elif hasattr(K, "ufl_shape") and len(K.ufl_shape) == 1 and K.ufl_shape[0] == mesh_dim**2:
        K_tensor = ufl.as_matrix([
            [K[i*mesh_dim+j] for j in range(mesh_dim)]
            for i in range(mesh_dim)
        ])
    else:
        raise ValueError(f"Unsupported shape for K: {getattr(K,'ufl_shape','unknown')}")
    expr = K_tensor * g
    return project_expression(expr, V, petsc_options=petsc_options)
\end{lstlisting}

\textbf{Replaces:}
\begin{lstlisting}[language=Python]
# Legacy inline tensor projection
myResults['vel1'] = project(-K1*grad(p1), Vvel,
                            solver_type='bicgstab',
                            preconditioner_type='petsc_amg')
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Encapsulate tensor–vector product projection.
  \item \textbf{Benefit:} Handles both full tensor and flattened cases.
  \item \textbf{Negative:} More complex helper logic for simple cases.
\end{itemize}

\subsubsection*{interpolate\_expression}
\begin{lstlisting}[language=Python]
def interpolate_expression(f_expr, V):
    """
    Nodal interpolation of UFL expression f_expr into V.
    """
    f = Function(V)
    expr = Expression(f_expr, V.element.interpolation_points())
    f.interpolate(expr)
    return f
\end{lstlisting}

\textbf{Replaces:}
\begin{lstlisting}[language=Python]
# Legacy projection used for scalar velocities
myResults['vel2'] = project(-K2*grad(p2), Vvel,
                            solver_type='bicgstab',
                            preconditioner_type='petsc_amg')
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Use fast, nodal‐interpolation instead of solve.
  \item \textbf{Benefit:} Balances accuracy with performance for scalar fields.
  \item \textbf{Negative:} Requires understanding of C++‐level interpolation.
\end{itemize}

\subsubsection*{interpolate\_tensor\_expression}
\begin{lstlisting}[language=Python]
def interpolate_tensor_expression(K, g, V):
    """
    Nodal interpolate the tensor–vector product K⋅g into V.
    """
    d = V.mesh.topology.dim
    if hasattr(K, "ufl_shape") and K.ufl_shape == (d, d):
        Kmat = K
    else:
        Kmat = ufl.as_matrix([
            [K[i*d+j] for j in range(d)]
            for i in range(d)
        ])
    return interpolate_expression(Kmat * g, V)
\end{lstlisting}

\textbf{Replaces:}
\begin{lstlisting}[language=Python]
# Legacy inline scalar interpolation for vel3
myResults['vel3'] = project(-K3*grad(p3), Vvel,
                            solver_type='bicgstab',
                            preconditioner_type='petsc_amg')
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Provide tensor‐aware nodal interpolation.
  \item \textbf{Benefit:} Supports both matrix and flattened tensor cases.
  \item \textbf{Negative:} Additional branching logic in helper.
\end{itemize}

\subsubsection*{interpolate\_to\_mesh\_order}
\begin{lstlisting}[language=Python]
def interpolate_to_mesh_order(func, mesh_order=1):
    """
    Re‐interpolate any Function to CG(mesh_order) for output.
    """
    mesh = func.function_space.mesh
    shape = func.ufl_shape
    cell = mesh.ufl_cell()._cellname
    rank = len(shape)
    if rank == 0:
        elt = basix.ufl.element("CG", cell, mesh_order)
    elif rank == 1:
        elt = basix.ufl.element("CG", cell, mesh_order, shape=(shape[0],))
    else:
        elt = basix.ufl.element("CG", cell, mesh_order, shape=shape)
    Vt = fem.functionspace(mesh, elt)
    f_interp = fem.Function(Vt)
    f_interp.interpolate(func)
    return f_interp
\end{lstlisting}

\textbf{Replaces:}
\begin{lstlisting}[language=Python]
# Legacy: no dedicated interpolation helper—functions were saved as‐is
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Ensure consistent CG discretization for all outputs.
  \item \textbf{Benefit:} Avoids mismatched function‐space orders.
  \item \textbf{Negative:} Adds overhead of element construction and re‐interpolation.
\end{itemize}

\subsubsection{compute\_integral\_quantities}

The function \texttt{compute\_integral\_quantities} required no structural changes when transitioning to DolfinX. The only update involved simplifying how default keyword arguments are handled. Specifically, the handling of the optional \texttt{save\_data} flag was modernized using Python’s built-in \texttt{dict.get()} method.

\begin{lstlisting}[language=Python, caption={Legacy default \texttt{save\_data} via branching}]
if 'save_data' in kwarg:
    save_data = kwarg.get('save_data')
else:
    save_data = True
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={DolfinX default \texttt{save\_data} via \texttt{get}}]
save_data = kwarg.get('save_data', True)
\end{lstlisting}


\subsection{finite\_element\_fcts}
the following functions are listed in the order in which they appear within the perfusion model run
through. Some of these functions are shared by the scripts in the perfusion folder.

\subsubsection{alloc\_fct\_spaces}

\begin{lstlisting}[language=Python, caption={Legacy: Manual kwarg extraction}]
if 'model_type' in kwarg:
    model_type = kwarg.get('model_type')
else:
    model_type = 'acv'
vel_order = kwarg.get('vel_order', fe_degr-1) if 'vel_order' in kwarg else fe_degr-1
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Pythonic defaulting via \texttt{dict.get}}]
model_type = kwargs.get("model_type", "acv")
vel_order   = kwargs.get("vel_order", fe_degr - 1)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Replace verbose conditional checks with \texttt{dict.get} defaults.
  \item \textbf{Benefit:} Reduces boilerplate and clarifies default values in one line.
  \item \textbf{Negative:} Legacy users must adapt to new parameter naming (\texttt{kwargs} vs \texttt{kwarg}).
\end{itemize}

\begin{lstlisting}[language=Python, caption={Legacy: No initialization logging}]
# (no print statements)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Rank-aware initialization logs}]
print0(f"alloc_fct_spaces: Initializing with fe_degr = {fe_degr}")
print0(f"alloc_fct_spaces: Mesh type = {cell_type}, dimension = {mesh_dim}")
print0(f"alloc_fct_spaces: Model type = '{model_type}', velocity order = {vel_order}")
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Add traceability for space allocation in parallel runs.
  \item \textbf{Benefit:} Aids debugging by reporting mesh and model configuration.
  \item \textbf{Negative:} Introduces extra output that may be noisy if overused.
\end{itemize}

\begin{lstlisting}[language=Python, caption={Legacy: Mixed element via \texttt{FiniteElement} \& \texttt{FunctionSpace}}]
P = FiniteElement('Lagrange', tetrahedron, fe_degr)
element = MixedElement([P, P, P])
Vp = FunctionSpace(mesh, element)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Mixed element via Basix and \texttt{fem.functionspace}}]
scalar_p = basix.ufl.element("Lagrange", cell_type, fe_degr)
mixed_p  = basix.ufl.mixed_element([scalar_p]*3)
Vp       = fem.functionspace(mesh, mixed_p)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Transition from legacy FEniCS element factories to Basix API.
  \item \textbf{Benefit:} Ensures consistency with DolfinX element definitions and future extensibility.
  \item \textbf{Negative:} Requires learning Basix’s element construction syntax.
\end{itemize}

\begin{lstlisting}[language=Python, caption={Legacy: Test/Trial split via \texttt{TestFunctions}/\texttt{split}}]
v_1, v_2, v_3 = TestFunctions(Vp)
p    = TrialFunction(Vp)
p_1, p_2, p_3 = split(p)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: UFL split via \texttt{ufl.TestFunction} and \texttt{ufl.split}}]
v        = ufl.TestFunction(Vp)
v_1,v_2,v_3 = ufl.split(v)
p        = ufl.TrialFunction(Vp)
p_1,p_2,p_3 = ufl.split(p)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Unify test/trial function API under UFL namespace in DolfinX.
  \item \textbf{Benefit:} Consistent use of UFL primitives and clearer separation of concerns.
  \item \textbf{Negative:} Slightly more abstract syntax compared to legacy helpers.
\end{itemize}

\begin{lstlisting}[language=Python, caption={Legacy: TensorFunctionSpace for K1}]
K1_space = TensorFunctionSpace(mesh, "DG", 0)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Mixed DG element for flattened tensor}]
k_scalar  = basix.ufl.element("DG", cell_type, 0)
k_tensor  = basix.ufl.mixed_element([k_scalar]*(mesh_dim*mesh_dim))
K1_space  = fem.functionspace(mesh, k_tensor)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Replace legacy tensor-space convenience with explicit flattened-element construction.
  \item \textbf{Benefit:} Supports general tensor shapes and aligns with Basix element model.
  \item \textbf{Negative:} More verbose and manual element assembly.
\end{itemize}

\begin{lstlisting}[language=Python, caption=Legacy: FunctionSpace for K2]
K2_space = FunctionSpace(mesh, "DG", 0)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={DolfinX: Basix DG element with \texttt{fem.functionspace}}]
K2_elem  = basix.ufl.element("DG", cell_type, 0)
K2_space = fem.functionspace(mesh, K2_elem)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Move to Basix element definitions for scalar permeability.
  \item \textbf{Benefit:} Consistent element creation pipeline within DolfinX.
  \item \textbf{Negative:} Must import and understand Basix API.
\end{itemize}

\begin{lstlisting}[language=Python, caption={Legacy: Velocity via \texttt{VectorFunctionSpace}}]
if vel_order == 0:
    Vvel = VectorFunctionSpace(mesh, "DG", vel_order)
else:
    Vvel = VectorFunctionSpace(mesh, "Lagrange", vel_order)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX: Single Basix element for velocity}]
vel_elem = basix.ufl.element("Lagrange" if vel_order>0 else "DG",
                              cell_type, vel_order, shape=(mesh_dim,))
Vvel     = fem.functionspace(mesh, vel_elem)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Consolidate velocity-space creation into a single element factory call.
  \item \textbf{Benefit:} Reduces branching and leverages Basix’s shape argument.
  \item \textbf{Negative:} Legacy users lose the explicit dual-branch clarity.
\end{itemize}

\subsubsection{set\_up\_fe\_solver2: ACV Model Differences}

\paragraph{Model type selection helper}
\begin{lstlisting}[language=Python, caption={Legacy ACV: multi-line model_type branch}]
if 'model_type' in kwarg:
    model_type = kwarg.get('model_type')
else:
    model_type = 'acv'
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX ACV: default via dict.get}]
model_type = kwarg.get("model_type", "acv")
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} More concise syntax for default arguments.
  \item \textbf{Benefit:} Idiomatic Python and less verbose code.
  \item \textbf{Negative:} None significant.
\end{itemize}

\paragraph{Inlet boundary file reading helper}
\begin{lstlisting}[language=Python, caption={Legacy ACV: read BC_data via np.loadtxt + nested ifs}]
if read_inlet_boundary == True:
    BC_data = np.loadtxt(inlet_boundary_file,skiprows=1,delimiter=',')
    if BC_data.ndim>1:
        b1 = 1000 * BC_data[:,1]
        boundary_labels = list(BC_data[:,0])
        n_labels = len(boundary_labels)
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX ACV: same read but vectorized b1 assignment}]
BC_data = np.loadtxt(inlet_boundary_file, skiprows=1, delimiter=',')
b1 = 1000 * BC_data[:,1] if BC_data.ndim > 1 else [1000 * BC_data[1]]
boundary_labels = list(BC_data[:,0]) if BC_data.ndim > 1 else [BC_data[0]]
n_labels = len(boundary_labels)
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Vectorize and flatten parsing logic for generality.
\end{itemize}

\paragraph{Dirichlet BC application helper}
\begin{lstlisting}[language=Python, caption={Legacy ACV: DirichletBC using V.sub and Constant}]
for i in range(n_labels):
    BCs.append( DirichletBC(V.sub(2), Constant(pv), boundaries, int(boundary_labels[i])) )
    if inlet_BC_type == 'DBC':
        BCs.append( DirichletBC(V.sub(0), Constant(BC_data[i,2]), boundaries, int(boundary_labels[i])) )
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX ACV: locate DOFs + fem.dirichletbc}] 
facet_indices = np.where(boundaries.values == boundary_id)[0]
boundary_facets = boundaries.indices[facet_indices]
dofs2 = fem.locate_dofs_topological(V.sub(2), mesh.topology.dim-1, boundary_facets)
BCs.append(fem.dirichletbc(fem.Constant(mesh, pv), dofs2, V.sub(2)))
# similar for DBC on compartment 0
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} DolfinX requires explicit topological DOF localization.
  \item \textbf{Benefit:} Ensures correct parallel DOF selection for BCs.
  \item \textbf{Negative:} More verbose than legacy \texttt{DirichletBC} shorthand.
\end{itemize}

\paragraph{Neumann BC integration helper}
\begin{lstlisting}[language=Python, caption={Legacy ACV: ds + assemble}] 
dS = ds(subdomain_data=boundaries)
area = assemble(Constant(1)*dS(boundary_id,domain=mesh))
integrals_N.append(b1[i]*v_1*dS(boundary_id))
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX ACV: ufl.dS + assemble_scalar + ufl.avg}]
area_form = fem.form(fem.Constant(mesh,1.0)*ufl.dS)
area = fem.assemble_scalar(area_form)
integrals_N.append(b1[i]*ufl.avg(v_1)*ufl.dS)
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Migrate to DolfinX parallel-safe assembly API.
  \item \textbf{Benefit:} Unified \texttt{fem.assemble_scalar} and consistent UFL usage.
  \item \textbf{Negative:} Requires wrapping forms and using \texttt{ufl.avg}.
\end{itemize}

\vspace{1em}
\subsubsection{set\_up\_fe\_solver2: A Model Differences}

\paragraph{Test/Trial setup helper}
\begin{lstlisting}[language=Python, caption={Legacy A: single-pressure split absent}] 
v_1, v_2, v_3 = TestFunctions(V)
p_1, p_2, p_3 = [], [], []
\end{lstlisting}
\begin{lstlisting}[language=Python, caption={DolfinX A: UFL TestFunction and None placeholders}]
v = ufl.TestFunction(V)
v_1 = v; v_2 = v_3 = None
p = ufl.TrialFunction(V)
p_1 = p; p_2 = p_3 = None
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Legacy used empty lists for unused components; DolfinX uses \texttt{None} for clarity.
  \item \textbf{Benefit:} Clearer semantic of unused test/trial slots.
  \item \textbf{Negative:} Slight API shift—users must handle \texttt{None} values.
\end{itemize}

\paragraph{Variational form helper}
\begin{lstlisting}[language=Python, caption={Legacy A: Direct single-compartment LHS/RHS}] 
LHS = inner(K1*grad(p),grad(v_1))*dx + beta_total*p*v_1*dx
RHS = sigma1*v_1*dx + sum(integrals_N) + beta_total*p_venous*v_1*dx
\end{lstlisting}
\begin{itemize}
  \item \textbf{Reason:} Leverage DolfinX’s form/assembly paradigm for performance and control.
  \item \textbf{Benefit:} Decouples form definition from assembly, enabling PETSc tuning.
  \item \textbf{Negative:} More boilerplate compared to legacy one-liner \texttt{solve}.
\end{itemize}

\subsubsection{Function \texttt{solve\_lin\_sys}}

This function solves a linear system with appropriate boundary conditions and solver settings. The DolfinX version introduces modularity, PETSc-native solver handling, boundary condition specialisation, and debugging tools. The transition reflects broader changes in DolfinX’s solver philosophy and API design.

\paragraph{1. Function Signature and Parameter Types}

\begin{lstlisting}[language=Python, caption={Legacy Dolfin}]
def solve_lin_sys(Vp,LHS,RHS,BCs,lin_solver,precond,rtol,mon_conv,init_sol,**kwarg):
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Modern DolfinX}]
def solve_lin_sys(V: dolfinx.fem.FunctionSpace, 
                  LHS: ufl.Form,
                  RHS: ufl.Form, 
                  BCs: list[dolfinx.fem.DirichletBC], 
                  lin_solver: str, 
                  precond: str, 
                  rtol: float, 
                  mon_conv: bool, 
                  init_sol: Optional[dolfinx.fem.Function],
                  inlet_BC_type: str, 
                  **kwargs) -> dolfinx.fem.Function:
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} DolfinX supports rich type annotations and encourages explicit typing to catch errors early.
  \item \textbf{Benefit:} Clearer contracts for arguments, improved IDE/static analysis support.
  \item \textbf{Drawback:} Slightly more verbose and less forgiving for quick experimentation.
\end{itemize}

\paragraph{2. Solver Configuration and Customisation}

\begin{lstlisting}[language=Python, caption={Legacy Dolfin}]
prm['linear_solver'] = lin_solver
prm['preconditioner'] = precond
prm['krylov_solver']["monitor_convergence"] = mon_conv
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Modern DolfinX}]
solver.setType(lin_solver)
solver.getPC().setType(precond)
solver.setTolerances(rtol=1e-15, atol=1e-10)
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} DolfinX uses direct PETSc interfaces instead of legacy parameter dictionaries.
  \item \textbf{Benefit:} Full control over solver and preconditioner; matches PETSc documentation exactly.
  \item \textbf{Drawback:} Requires more familiarity with PETSc's solver API and options.
\end{itemize}

\paragraph{3. Boundary Condition Handling and Lifting}

\begin{lstlisting}[language=Python, caption={Legacy Dolfin}]
problem = LinearVariationalProblem(LHS, RHS, p, BCs)
solver = LinearVariationalSolver(problem)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Modern DolfinX}]
A = dolfinx.fem.petsc.assemble_matrix(LHS, bcs=BCs)
b = dolfinx.fem.petsc.assemble_vector(RHS)
dolfinx.fem.petsc.apply_lifting(b, [LHS], [BCs])
dolfinx.fem.set_bc(b, BCs)
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} DolfinX separates matrix/vector assembly and boundary condition application.
  \item \textbf{Benefit:} More flexible and transparent handling of BCs, necessary for mixed or Neumann problems.
  \item \textbf{Drawback:} More lines of code required to achieve the same effect.
\end{itemize}

\paragraph{4. NBC and DBC-Specific Solver Strategies}

\begin{lstlisting}[language=Python, caption={Modern DolfinX Only}]
if bc_type == "NBC":
    solver.setType("bcgs")
    solver.getPC().setType("hypre")
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} Different PDE types (e.g., Neumann vs. Dirichlet) benefit from different solvers/preconditioners.
  \item \textbf{Benefit:} More robust convergence for under-determined or ill-conditioned Neumann problems.
  \item \textbf{Drawback:} Adds conditional complexity; requires tuning for each system.
\end{itemize}

\paragraph{5. Monitoring and Debugging Tools}

\begin{lstlisting}[language=Python, caption={Modern DolfinX Only}]
def ksp_monitor(ksp, it, rnorm):
    print0(f"Iteration {it}: Residual norm = {rnorm:.3e}")
solver.setMonitor(ksp_monitor)
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} PETSc provides low-level hooks for convergence diagnostics.
  \item \textbf{Benefit:} Helps identify stagnation or divergence in Krylov methods.
  \item \textbf{Drawback:} Can clutter output unless selectively enabled.
\end{itemize}

\paragraph{6. Return Type and Output}

\begin{lstlisting}[language=Python, caption={Legacy Dolfin}]
return p
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Modern DolfinX}]
return ufl.Function(V).vector().update(x)
\end{lstlisting}

\begin{itemize}
  \item \textbf{Reason:} In DolfinX, the `Function` and PETSc `Vec` must be explicitly connected.
  \item \textbf{Benefit:} Maintains clear separation between mathematical objects and solver state.
  \item \textbf{Drawback:} Users must manually wrap solver output into solution fields.
\end{itemize}

\section{Summary of Modernization of the Cerebral Perfusion Solver}

The modernization of the cerebral perfusion finite element solver from FEniCS to FEniCSx represents a significant advancement aimed at enhancing the solver's efficiency, scalability, and compatibility with contemporary parallel computing frameworks. This transition also facilitates the integration of more sophisticated boundary condition handling, robust solver configurations, and improved performance through parallelism and adaptive refinement techniques.

\subsection{Adapting the Solver to FEniCSx}

The migration from FEniCS to FEniCSx is central to this modernization effort. Key components of the solver, including mesh handling, function spaces, and linear algebra operations, were refactored to align with the FEniCSx API. This transition necessitated adjustments to the assembly process of the linear system, the formulation of the weak forms, and the utilization of PETSc for linear solvers, as FEniCSx offers enhanced capabilities in parallel computation with MPI and improved memory handling \cite{dokken2021dolfinx}.

\begin{itemize} \item \textbf{Mesh Handling}: FEniCSx introduces the \texttt{dolfinx} namespace for mesh and function space management. The solver was updated to support the new mesh types and associated finite element spaces in \texttt{dolfinx.fem}. \item \textbf{Linear Algebra}: The solver now fully integrates with PETSc’s parallel linear solver capabilities, providing significant performance improvements for large-scale simulations. This includes the configuration of Krylov solvers and preconditioners through \texttt{KSP} and \texttt{PC} classes in PETSc \cite{balay2022petsc}. \end{itemize}

\subsection{Boundary Condition Management}

An important aspect of this modernization was the implementation of a more flexible and robust boundary condition (BC) handling system. The solver was extended to support a variety of boundary conditions, including Dirichlet, Neumann, and mixed boundary conditions, which are now dynamically handled based on input configurations. The BCs are applied directly during the assembly process, with special attention to the lifting operations required to enforce the Dirichlet conditions \cite{dokken2021dolfinx}.

\begin{itemize} \item \textbf{Inlet BCs}: A system for specifying inlet boundary conditions based on artery occlusion status (healthy vs. occluded) was developed. The BC setup accounts for different physiological conditions, which is crucial for simulating the effects of ischemic stroke in cerebral perfusion models. \item \textbf{Pressure Anchoring}: In certain cases, pressure anchoring was implemented to ensure the correct solution of pressure fields in regions with no flow or when specific pressure values need to be prescribed. \end{itemize}

\subsection{Parallelization and Performance Enhancements}

The solver was parallelized using MPI to efficiently handle large 3D simulations. The modernization ensures that critical computations, such as the linear solve, are parallelized across multiple processor cores, while other components (e.g., setup, configuration parsing) remain serial to avoid unnecessary overhead \cite{dokken2021dolfinx}.

\begin{itemize} \item \textbf{Adaptive Solvers}: Although MPI parallelization is implemented, adaptive solver strategies were considered for further refinement in solving accuracy. The implementation leverages PETSc’s Krylov solvers and can monitor convergence during the solution process. \item \textbf{Solver Flexibility}: The solver now supports different solver types (e.g., GMRES, BCGS, CG) and preconditioners (e.g., Jacobi, Hypre) to optimize performance based on the problem's characteristics and the hardware available \cite{balay2022petsc}. \end{itemize}

\subsection{Enhanced Solver Configuration and Monitoring}

A significant feature of the modernized solver is the flexible solver configuration that allows for fine-tuning of solver parameters, such as relative tolerance (\texttt{rtol}), convergence monitoring, and solver types. The solver can now be customized based on the type of boundary conditions and the characteristics of the model being simulated.

\begin{itemize} \item \textbf{Convergence Monitoring}: Convergence monitoring is now incorporated, providing real-time feedback during the solver’s execution. This is particularly valuable for identifying slow convergence and optimizing solver parameters dynamically. \item \textbf{Solver Diagnostics}: Enhanced debugging and diagnostic tools have been integrated, allowing for detailed insights into the solver's behavior, such as the residual norm, solver iterations, and pressure field evolution. \end{itemize}

\subsection{Function Space and Solution Management}

The solution process was also enhanced with better management of function spaces and solution vectors. The solver ensures that the solution is accurately projected into the correct finite element function space, and updates are made to the ghost values to ensure consistency across distributed processors \cite{dokken2021dolfinx}.

\begin{itemize} \item \textbf{Function Space Compatibility}: Compatibility checks between the left-hand side (LHS) and right-hand side (RHS) function spaces ensure that the solver operates on correctly matched spaces, reducing errors during matrix assembly. \item \textbf{Solution Projection}: Once the system is solved, the solution is projected back into the finite element function space, ensuring that the pressure fields are accurately represented for further analysis and visualization. \end{itemize}

This modernization effort has significantly enhanced the solver's efficiency, flexibility, and scalability. The integration with FEniCSx, coupled with MPI parallelization and advanced solver configurations, ensures that the solver is ready to handle more complex and larger-scale cerebral perfusion simulations with better performance and accuracy.
\newpage

\section{Weak vs Strong scaling: FEniCSx vs FEniCSx Legacy 'a' model}
The transition from the legacy FEniCS platform to the modern FEniCSx framework introduces significant changes not only in code syntax and structure but also in the parallel performance characteristics of core solvers and modules. This section investigates both \textbf{weak scaling} and \textbf{strong scaling} properties of key components within the cerebral perfusion model pipeline, comparing FEniCSx to its predecessor. Weak scaling tests the framework's efficiency as the problem size grows proportionally with the number of processors, while strong scaling assesses performance improvements for a fixed problem size as more processors are employed, we focus on the basic flow solver.
\subsection{Methodology}

\subsubsection{Automated Performance Benchmarking}

\paragraph{Objective}
To evaluate the parallel performance of the modernised DolfinX-based multiphase cerebral perfusion solver, we developed an automated benchmarking and post-processing pipeline. This framework systematically varies key simulation parameters including:
\begin{itemize}
    \item Model type (e.g., \texttt{a}, \texttt{acv}),
    \item Finite element degree,
    \item Velocity order,
    \item Inlet boundary condition type (Dirichlet, Neumann, mixed),
    \item Inlet boundary extraction method (e.g., geometry-based or label-based).
\end{itemize}

\paragraph{Execution Strategy}
Simulations were launched using a shell loop or a \texttt{joblib.Parallel}-based Python controller. For each configuration, the following steps were executed:
\begin{enumerate}
    \item Generate and validate a custom YAML configuration file.
    \item Run the solver under varying MPI processor counts (\texttt{-np} = 1, 2, 4, 8, 16, 32).
    \item Measure wall-clock execution time using high-resolution timers.
    \item Log runtime metadata to \texttt{simulation\_summary.csv}, and compute speed-up and efficiency relative to serial baselines.
\end{enumerate}

\paragraph{Data Management}
Each run's metadata was stored in structured CSV files:
\begin{itemize}
    \item \texttt{simulation\_summary.csv} — parameters, wall time, status.
    \item \texttt{speedup\_analysis.csv} — same data, with added speedup and efficiency metrics.
\end{itemize}

\subsubsection{Post-Processing and Visualization}

\paragraph{Plot Generation}
A dedicated Python script using \texttt{pandas}, \texttt{matplotlib}, and \texttt{seaborn} was developed to generate the following performance plots:
\begin{itemize}
    \item Execution Time vs. Number of Processors
    \item Parallel Speedup vs. Number of Processors
    \item Parallel Efficiency vs. Number of Processors
\end{itemize}

Each unique configuration (boundary condition, read method, velocity order) is represented by a distinct color and marker, and a standalone legend is saved for use in publications.

\paragraph{Example Snippet}
\begin{lstlisting}[language=Python, caption={Data Filtering and Grouped Plotting}]
df = pd.read_csv("efficiency_analysis.csv")
df = df[(df["status"] == "success") & df["time_seconds"].notnull()]
plot_group(df_sub, "efficiency", "Parallel Efficiency", ...)
\end{lstlisting}

\paragraph{Export and Styling}
All plots were saved using \texttt{tight\_layout()} to avoid clipping of labels, and styled using the \texttt{whitegrid} theme. Fonts were scaled to suit academic publication standards.

\subsubsection{Weak Scaling Methodology}

To evaluate the weak scaling performance of the basic flow solver, we followed a classical methodology in which the problem size \( N \) grows proportionally with the number of processors \( P \), keeping the per-processor workload constant. The runtime \( T(P) \) is then compared to the single-processor baseline \( T(1) \) using:

\[
\text{Efficiency}_{\text{weak}}(P) = \frac{T(1)}{T(P)}
\]

This allows for a fair assessment of whether the solver scales effectively when both problem size and resource count increase simultaneously.

\paragraph{Procedure}
\begin{enumerate}
    \item Choose a base mesh yielding an appropriate DOF count \( N_0 \) for a single processor.
    \item Increase the number of processors \( P \) while refining or duplicating the mesh to ensure \( N = P \cdot N_0 \).
    \item Run the same solver setup for each configuration and record wall-clock times.
    \item Compute parallel efficiency and generate comparative plots.
\end{enumerate}

\paragraph{DOF Distribution in Tetrahedral Elements}
The number of degrees of freedom per cell depends on the Lagrange polynomial degree. This influences the required number of processors under weak scaling:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|l|}
\hline
\textbf{FE Degree} & \textbf{DOFs per Cell} & \textbf{Topological Entities} \\
\hline
1 (P1) & 4  & Vertices only \\
2 (P2) & 10 & Vertices + Edges \\
3 (P3) & 21 & Vertices, Edges, Faces, Cell Interior \\
\hline
\end{tabular}
\caption{DOF distribution for Lagrange elements on a tetrahedral mesh}
\end{table}

\paragraph{Processor Matching Under Constant Workload}
To maintain a fixed DOF-per-processor ratio, the number of processors must scale proportionally with the DOFs per cell. For example, if 1 processor is used for FE1 (4 DOFs), then:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{FE Degree} & \textbf{DOFs per Cell} & \textbf{Processors Required} \\
\hline
1 (P1) & 4  & 1 \\
2 (P2) & 10 & 3 \\
3 (P3) & 21 & 6 \\
\hline
\end{tabular}
\caption{Processor counts for maintaining constant DOFs per processor under weak scaling}
\end{table}
The number of processors is determined based on the number of DOFs per cell, ensuring that each processor handles an equivalent amount of work across different FE degrees. This matching of processors with problem size allows for an efficient scaling analysis.




\subsubsection{Strong Scaling Methodology}

To evaluate the strong scaling performance of the modernized DolfinX-based multiphase cerebral perfusion solver, simulations were performed using all combinations of the following parameters:
\begin{itemize}
    \item \textbf{Model Type:} \texttt{a}, \texttt{acv}
    \item \textbf{Boundary Condition Type:} \texttt{DBC}, \texttt{NBC}, \texttt{mixed}
    \item \textbf{Inlet Boundary Extraction:} \texttt{true}
    \item \textbf{Finite Element Degree:} 1, 2, 3
    \item \textbf{Velocity Order:} 1, 2, 3
    \item \textbf{Number of Processors:} 1, 2, 4, 8, 16
\end{itemize}

For each parameter combination:
\begin{enumerate}
    \item A custom YAML configuration file was generated.
    \item The solver was run on a fixed-size mesh with varying MPI processor counts.
    \item Wall-clock time was measured, and data was logged in \texttt{simulation\_summary.csv}.
    \item Speed-up was computed relative to the serial execution (\texttt{np=1}).
\end{enumerate}

The speed-up for strong scaling was calculated as:
\[
\text{Speedup}(P) = \frac{T(1)}{T(P)}
\]
where \( T(1) \) is the time for 1 processor (baseline) and \( T(P) \) is the time for \( P \) processors.

\subsubsection{Interpretation}

The scaling performance of the multiphase cerebral perfusion solver was evaluated through both weak and strong scaling analyses.

\paragraph{Weak Scaling:}
In weak scaling, the problem size \( N \) was increased proportionally with the number of processors \( P \), keeping the per-processor workload constant. The weak scaling efficiency was computed using:

\[
\text{Efficiency}_{\text{weak}}(P) = \frac{T(1)}{T(P)}
\]

Where \( T(1) \) is the time for a single processor, and \( T(P) \) is the time for \( P \) processors. Results showed that the solver's efficiency decreased with increasing processor count for lower finite element degrees (e.g., FE degree 1), while higher-order simulations (e.g., FE degree 3) demonstrated better scalability, with efficiency remaining above 0.75 even as processor count increased.

\paragraph{Strong Scaling:}
In strong scaling, the problem size was kept constant while the number of processors was increased. The speed-up relative to a baseline (single processor) was calculated as:

\[
\text{Speedup}(P) = \frac{T(1)}{T(P)}
\]

The strong scaling results showed that for small problem sizes (e.g., FE degree 1), the solver experienced limited speed-up as processor count increased, reflecting overheads and insufficient work per processor. However, for larger problem sizes (e.g., FE degree 3), speed-up improved significantly with increasing processor count, especially when the number of processors was appropriate for the DOF distribution (e.g., 6 processors for FE degree 3).

\newpage
\subsection{Results for 'a' model}

\subsubsection{Weak Scaling}
In this section, we present the results of our weak scaling analysis for the perfusion simulation across various models, boundary conditions, and finite element degrees. The scaling efficiency is examined by comparing execution times and weak scaling efficiency for different numbers of processors (NP).

The plot in Figure \ref{fig:eff} demonstrates the weak scaling efficiency for the three boundary conditions (DBC, NBC, and mixed) as the number of processors increases. As expected, the efficiency decreases as the problem size increases, especially for higher degrees of freedom (i.e., FE2 and FE3), due to the overhead of parallelization.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/efficiency_vs_np_with_lines.png}
    \caption{Weak scaling efficiency as a function of the number of processors for different boundary conditions (DBC, NBC, mixed) and finite element degrees.}
    \label{fig:eff}
\end{figure}

The following table summarizes the execution time and weak scaling efficiency for each combination of model, boundary condition, finite element degree, and processor count.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{BC Type} & \textbf{FE Degree} & \textbf{NP} & \textbf{Time (s)} & \textbf{Weak Scaling Efficiency} \\
\hline
DBC & 1 & 1 & 66.3249 & 1 \\
DBC & 2 & 3 & 260.7634 & 0.0848 \\
DBC & 3 & 6 & 886.2843 & 0.0125 \\
NBC & 1 & 1 & 97.7070 & 1 \\
NBC & 2 & 3 & 380.9875 & 0.0855 \\
NBC & 3 & 6 & 2176.3225 & 0.0075 \\
mixed & 1 & 1 & 68.0358 & 1 \\
mixed & 2 & 3 & 260.4852 & 0.0871 \\
mixed & 3 & 6 & 1108.4242 & 0.0102 \\
\hline
\end{tabular}
\caption{Execution time and weak scaling efficiency for different models, boundary conditions, finite element degrees, and processor counts for 'a' model.}
\label{tab:scaling_results}
\end{table}

The results show that as the finite element degree increases (from FE1 to FE3), the efficiency declines significantly. This behavior can be attributed to the increasing computational overhead associated with the larger problem sizes as the number of processors is scaled up.

The weak scaling efficiency results for each boundary condition indicate that the parallelization strategy works best for smaller problem sizes (FE1) with a significant drop in efficiency as the problem size grows. This suggests that the implementation may need optimization for larger FE degrees, especially when using high processor counts, to improve the scalability of the solver. Further investigation into load balancing and communication overhead may provide opportunities to enhance the parallel efficiency in future implementations.


\subsubsection{Strong scaling}
Strong scaling evaluations of the basic flow solver fix the total mesh size and increase the processor count to assess how efficiently the solver reduces runtime. This subsection discusses improvements in solver speed and reductions in wall-clock time, balanced against the overhead of increased communication and potential degradation in preconditioner performance. Comparisons highlight how the modern back end of FEniCSx affects scalability compared to the more monolithic solver design in legacy FEniCS.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/legend_only.png}
    \caption{legend}
    \label{fig:enter-label}
\end{figure}

\newpage

\begin{figure}[!htbp]
  \centering

  % First row: model=a, FE=1
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/time_vs_np_a_FE1.png}
    \caption{Execution Time: Model a, FE=1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/speedup_vs_np_a_FE1.png}
    \caption{Speed-Up: Model a, FE=1}
  \end{subfigure}
  \caption{Weak scaling performance across models and finite element degrees.}
\end{figure}

We observe that the weak scaling performance for all simulations using finite element degree 1-regardless of the velocity order (first, second, or third)-is suboptimal. Specifically, as the number of processors increases, the execution time also increases. This trend suggests that the benefits of parallelism are outweighed by communication overheads, or that the problem size (mesh resolution) is not sufficiently large to saturate the additional compute resources. 

Furthermore, the computed speedups confirm this inefficiency, as they deviate significantly from ideal linear scaling. These results indicate that, under the current configuration, allocating additional processors does not yield a performance benefit and may in fact reduce computational efficiency.

\begin{figure}[!htbp]
  \centering
  % Second row: model=a, FE=2
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/time_vs_np_a_FE2.png}
    \caption{Execution Time — Model a, FE=2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/speedup_vs_np_a_FE2.png}
    \caption{Speed-Up — Model a, FE=2}
  \end{subfigure}
  \caption{Weak scaling performance across models and finite element degrees.}
\end{figure}


We observe an improved scaling behavior when using finite element degree 2 compared to degree 1. In particular, there is a measurable reduction in execution time as the number of processors increases, indicating that the computational workload is more effectively distributed in this configuration.

Across most simulation groups, the greatest gains are observed when scaling from 1 to 2 processors, with execution times reduced by approximately $124.4268$s. An exception is noted for simulations employing Neumann boundary conditions (NBC), which continue to benefit from parallelism up to 4 processors, achieving a maximum speedup of $2.0685$.

However, as the number of processors increases further to 8 and 16, the efficiency gains diminish. For example, while the NBC group remains faster with 8 processors than in serial execution, performance degrades when using 16 processors, likely due to increased communication overheads outweighing computational benefits. This suggests that there exists an optimal processor count beyond which additional resources may hinder performance for this problem scale.



\begin{figure}[!htbp]
  \centering
  % Third row: model=acv, FE=1
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/time_vs_np_a_FE3.png}
    \caption{Execution Time — Model acv, FE=3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/speedup_vs_np_a_FE3.png}
    \caption{Speed-Up — Model a, FE=3}
  \end{subfigure}
  
  \caption{Weak scaling performance across models and finite element degrees.}
\end{figure}

For simulations using finite element degree 3, we observe further improvements in processor scaling and speedup compared to lower-degree cases. Execution times for serial runs are significantly higher, with an average of approximately 7724.1778s, reflecting the increased computational cost associated with higher polynomial order.

The Neumann boundary condition (NBC) simulations once again exhibit the highest execution times in serial, taking over twice as long as their Dirichlet (DBC) and mixed boundary condition counterparts on average. This is consistent with previous observations and may be attributed to the nature of the boundary implementation and solver behavior.

In terms of parallel performance, the most substantial reduction in execution time for the DBC and mixed cases is observed when using 8 processors, with an average speedup of 4.849 and 4.7012 respectively. For the NBC simulations, however, the optimal configuration appears to be 16 processors, where execution time is reduced most effectively compared to all other tested processor counts with an avarage speed up of 7.290. This highlights the improved scalability of the FE degree 3 configuration, particularly under more complex boundary conditions.


\newpage

\subsection{Results for 'acv' model}


\subsubsection{Weak Scaling}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{BC Type} & \textbf{Read Inlet} & \textbf{FE Degree} & \textbf{Status} & \textbf{Time (s)} & \textbf{Efficiency} \\
\hline
DBC   & true & 1 & 1 & 112.62 & 1.000 \\
DBC   & true & 2 & 3 & 1198.01 & 0.031 \\
NBC   & true & 1 & 1 & 149.59 & 1.000 \\
NBC   & true & 2 & 3 & 1872.27 & 0.027 \\
mixed & true & 1 & 1 & 116.48 & 1.000 \\
mixed & true & 2 & 3 & 1197.22 & 0.032 \\
\hline
\end{tabular}
\caption{Strong scaling efficiency for the 'acv' model under various configurations}
\end{table}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/efficiency_vs_np_with_lines_acv.png}
    \caption{Weak scaling efficiency as a function of the number of processors for different boundary conditions (DBC, NBC, mixed) and finite element degrees.}
    \label{fig:acveff}
\end{figure}


The table and figure above illustrate the weak scaling behaviour of the \texttt{acv} model across varying boundary condition types (Dirichlet, Neumann, and mixed), finite element degrees, and processor counts. In each case, the number of degrees of freedom increased proportionally with the number of processors, allowing assessment of how well the computational cost is distributed in a weak scaling regime.

We observe that for all boundary condition types, the efficiency at finite element degree 1 and a single processor is normalized to 1.000, serving as the baseline. When increasing the FE degree to 2 and running on three processors, the efficiency drops significantly across all boundary conditions: to approximately 3.1\% for DBC, 2.7\% for NBC, and 3.2\% for mixed. This drop indicates a notable overhead introduced either by increased communication cost between processors or poor scaling in the linear solver phase.

The relatively consistent drop in efficiency across all boundary condition types suggests that the dominant bottleneck lies in the finite element complexity and not in the boundary condition imposition itself. The rise in computational demand with higher FE degree likely amplifies the imbalance in workload distribution or exacerbates synchronization costs during parallel assembly and solution.

Overall, the weak scaling results reveal that while the model executes successfully across all tested configurations, its parallel efficiency remains limited under increased polynomial order and processor counts. This highlights a potential area for optimization, either by improving domain decomposition strategies or by incorporating more scalable solvers and preconditioners tailored for high-order elements


\subsubsection{Strong scaling}
Strong scaling evaluations fix the total mesh size for the \texttt{acv} model and increase the processor count to assess solver efficiency. This analysis examines the evolution of wall‐clock time and speed‐up across different boundary conditions (DBC, NBC, mixed) and finite element degrees (FE=1, FE=2), highlighting diminishing returns when communication overhead and preconditioner degradation dominate.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/time_vs_np_acv_FE1.png}
    \caption{Execution Time: Model \texttt{acv}, FE=1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/speedup_vs_np_acv_FE1.png}
    \caption{Speed‐Up: Model \texttt{acv}, FE=1}
  \end{subfigure}
  \caption{Strong scaling performance for finite element degree 1.}
  \label{fig:strong_acv_fe1}
\end{figure}

At FE=1, all boundary‐condition groups exhibit sub‐optimal scaling: execution time increases monotonically with processor count, and speed‐up values fall well below ideal linear behavior. For example, the DBC group sees time grow from 112.6\,s (1 core) to 606.9\,s (16 cores), with a maximum speed‐up of only 0.80 on 2 cores. Similar trends for NBC and mixed cases indicate that the mesh resolution is too coarse to amortize communication costs, in agreement with Amdahl’s law on limited parallel fraction \cite{amdahl1967validity}.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/time_vs_np_acv_FE2.png}
    \caption{Execution Time: Model \texttt{acv}, FE=2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Figures/speedup_vs_np_acv_FE2.png}
    \caption{Speed‐Up: Model \texttt{acv}, FE=2}
  \end{subfigure}
  \caption{Strong scaling performance for finite element degree 2.}
  \label{fig:strong_acv_fe2}
\end{figure}

With FE=2, parallel efficiency improves markedly for small core counts: DBC speed‐up reaches 3.30 on 4 cores, NBC attains 4.48, and mixed achieves 2.98, consistent with increased computation‐to‐communication ratio (Gustafson’s insight) \cite{gustafson1988reevaluating}. However, efficiency declines beyond 4–8 cores: e.g., NBC drops from 4.48 at 4 cores to 4.03 at 16 cores, evidencing the dominance of communication latency and preconditioner scalability limits \cite{gropp1999mpi}. This suggests an optimal core count around 4–8 for FE=2 under the current mesh size.



\subsection{Conclusion}

In this study, we investigated both the weak and strong scaling performance of a cerebral perfusion simulation using different models, boundary conditions, and finite element degrees. Our results reveal a mixed performance across scaling tests. While the strong scaling results show a positive trend for larger finite element (FE) degrees (FE2 and FE3), with reductions in execution time as processor counts increase, the weak scaling performance remains poor, especially for higher FE degrees.

The poor weak scaling efficiency suggests that the overhead associated with parallelization grows disproportionately as the problem size increases. This may indicate that the problem is not scaling efficiently with the increasing number of processors, potentially due to excessive communication overhead or load imbalance in the parallel distribution of the computational work. As the finite element degree increases, the problem size becomes larger, and this imbalance may become more pronounced, leading to diminishing returns on parallel performance.

On the other hand, the positive strong scaling observed for higher FE degrees indicates that while weak scaling is struggling, the parallel solver does benefit from increased processor counts when the problem size is fixed. This is especially notable for FE2 and FE3, where larger problem sizes can better utilize the additional computational resources, leading to faster execution times. This suggests that the solver is able to handle larger workloads more efficiently, but the challenge lies in distributing the computational work and data efficiently across processors as the problem size scales up.

These findings suggest that the parallel implementation is effective for larger FE degrees in strong scaling, but the weak scaling limitations indicate a need for optimization. Specifically, better load balancing, improved communication strategies, and more efficient solver techniques could help address the poor weak scaling observed in larger simulations. The combination of these optimizations may allow for better overall scalability, particularly as simulations grow in complexity with higher finite element degrees.

In summary, while the solver performs well in terms of strong scaling for larger problem sizes, further work is needed to improve weak scaling efficiency to ensure optimal performance as both the number of processors and the problem size grow. Optimizing for weak scaling will be essential to fully leverage the parallelism potential of the solver for complex, high-degree simulations.





\newpage


\section{Future work}
Several avenues remain to enhance the cerebral perfusion framework as it migrates from legacy FEniCS to modern FEniCSx:

\begin{itemize}
  \item \textbf{Memory‐efficient high‐order discretizations.}  
    Investigate hierarchical matrices, blocked sparse‐storage formats, or in‑situ compression to enable FE=3 and third‐order velocity spaces without exceeding memory limits.  

  \item \textbf{Rigorous validation of \texttt{Gem\_x} utilities.}  
    Develop unit, integration, and regression tests (e.g.\ with \texttt{pytest}) and benchmark against analytical solutions or high‐fidelity data to ensure correctness and robustnes.  
    
  \item \textbf{Scalable preconditioning for high‐order elements.}  
    Integrate algebraic multigrid techniques (e.g.\ BoomerAMG, GAMG) tailored to high‐order FE spaces to reduce solver setup costs and improve weak scaling \cite{yang2006large}.  

  \item \textbf{Dynamic load balancing.}  
    Employ Zoltan’s graph‐ and hypergraph‐based repartitioning to mitigate load imbalance and communication hotspots in complex geometries \cite{devine2006zedg}.  

  \item \textbf{Heterogeneous acceleration.}  
    Explore GPU offload of assembly and linear solves via libraries such as hypre’s GPU kernels or PETSc’s GPU backends to further reduce time‐to‐solution \cite{brown2012gpu}.  

    
\end{itemize}
\newpage
\printbibliography




\end{document}
